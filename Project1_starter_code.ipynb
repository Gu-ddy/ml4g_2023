{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that are required to run your project\n",
    "# You are allowed to add more libraries as you need\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PATHS\n",
    "\n",
    "# NOTE: \n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you. \n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "# Path for datasets\n",
    "path_cwd = os.getcwd()\n",
    "path_data = path_cwd+\"/ML4G_Project_1_Data\"\n",
    "\n",
    "# Metadata for genes of cell lines X1 and X2\n",
    "train_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_info.tsv\"\n",
    "train_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_info.tsv\"\n",
    "val_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_info.tsv\"\n",
    "val_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_info.tsv\"\n",
    "\n",
    "# Gene expression values for cell lines X1 and X2\n",
    "train_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_y.tsv\"\n",
    "train_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_y.tsv\"\n",
    "val_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_y.tsv\"\n",
    "val_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_y.tsv\"\n",
    "\n",
    "# DNase and histone modification data for cell lines X1, X2 and X3\n",
    "bed_files_X1 = [\"/DNase-bed/X1.bed\",\n",
    "                \"/H3K4me1-bed/X1.bed\",\n",
    "                \"/H3K4me3-bed/X1.bed\",\n",
    "                \"/H3K9me3-bed/X1.bed\",\n",
    "                \"/H3K27ac-bed/X1.bed\",\n",
    "                \"/H3K27me3-bed/X1.bed\",\n",
    "                \"/H3K36me3-bed/X1.bed\"]\n",
    "bed_file_paths_X1 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X2 = [\"/DNase-bed/X2.bed\",\n",
    "                \"/H3K4me1-bed/X2.bed\",\n",
    "                \"/H3K4me3-bed/X2.bed\",\n",
    "                \"/H3K9me3-bed/X2.bed\",\n",
    "                \"/H3K27ac-bed/X2.bed\",\n",
    "                \"/H3K27me3-bed/X2.bed\",\n",
    "                \"/H3K36me3-bed/X2.bed\"]\n",
    "bed_file_paths_X2 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X3 = [\"/DNase-bed/X3.bed\",\n",
    "                \"/H3K4me1-bed/X3.bed\",\n",
    "                \"/H3K4me3-bed/X3.bed\",\n",
    "                \"/H3K9me3-bed/X3.bed\",\n",
    "                \"/H3K27ac-bed/X3.bed\",\n",
    "                \"/H3K27me3-bed/X3.bed\",\n",
    "                \"/H3K36me3-bed/X3.bed\"]\n",
    "bed_file_paths_X3 = [path_data+file for file in bed_files_X1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "### FUNCTION FOR EXTRACTION OF FEATURES\n",
    "def extract_features(bed_path, info_path, max_distance, resolution, stride):\n",
    "    \"\"\"\n",
    "    Function extracting binary features from bed datasets\n",
    "    :param bed_path: path to bed file of interest\n",
    "    :param info_path: path to info file of interest\n",
    "    :param max_distance: maximal distance from TSS that should be considered\n",
    "    :param resolution: window size of aggregation for dimensionality reduction\n",
    "    :param stride: stride for dimensionality reduction\n",
    "    :return: pandas df of type int8 containing binary features\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df_info = pd.read_csv(info_path, sep='\\t', usecols=[0,1,4])\n",
    "    df_peak_data = pd.read_csv(bed_path, sep='\\t', usecols=[0,1,2], names = [\"chromosome\", \"peak_start\", \"peak_end\"])\n",
    "\n",
    "    # Get genes and initialize features df with False as entries\n",
    "    df_features = pd.DataFrame(data=0,columns=[i-max_distance-1 for i in range(1, 2*(max_distance+1))], index=df_info[\"gene_name\"], dtype=\"int8\")\n",
    "\n",
    "    # Fill df according to info data\n",
    "    for i in df_info.index:\n",
    "        gene = df_info[\"gene_name\"][i]\n",
    "        tss = df_info[\"TSS_start\"][i]\n",
    "        chromosome = df_info[\"chr\"][i]\n",
    "        tss_l = tss - max_distance\n",
    "        tss_r = tss + max_distance\n",
    "\n",
    "        # Print progress\n",
    "        if i == 0:\n",
    "            print(\"Start preprocessing of:\", \"\\n\"+\n",
    "                  \"Dataset:\", bed_path, \"\\n\"+\n",
    "                  \"Infoset:\", info_path)\n",
    "        if i == df_info.index[-1]:\n",
    "            print(\"Done!\" + \"\\n\" + \"-----------------------------------\")\n",
    "\n",
    "        # Find relevant peaks\n",
    "        peaks = df_peak_data.loc[(df_peak_data[\"peak_start\"] < tss_r) &\n",
    "                                 (df_peak_data[\"peak_end\"] > tss_l)]\n",
    "\n",
    "        # Fill features dataset\n",
    "        for j in range(peaks.shape[0]):\n",
    "            # Make sure that peak is on the same chromosome\n",
    "            if peaks[\"chromosome\"].iloc[j] != chromosome: continue\n",
    "\n",
    "            # Get peak boundaries\n",
    "            peak_l = peaks[\"peak_start\"].iloc[j]\n",
    "            peak_r = peaks[\"peak_end\"].iloc[j]\n",
    "\n",
    "            # Consider possible cases\n",
    "            if (peak_l >= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : peak_r-tss] = 1\n",
    "\n",
    "            elif (peak_l <= tss_r) and (peak_r >= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : tss_r-tss] = 1\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : peak_r-tss] = 1\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : tss_r-tss] = 1\n",
    "\n",
    "    # Introduce resolution (rather inefficient...)\n",
    "    df_features=df_features.rolling(window=resolution,\n",
    "                                      axis=1,\n",
    "                                      step=stride,\n",
    "                                      min_periods=1).mean()\n",
    "\n",
    "    df_features[df_features > 0.5] = 1\n",
    "    df_features[df_features <= 0.5] = 0\n",
    "\n",
    "    return df_features.astype(\"int8\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "### FUNCTION FOR CREATING TRAINING DATASET\n",
    "def create_set(bed_paths, df_info, max_distance, resolution, stride):\n",
    "    \"\"\"\n",
    "    Create training dataset\n",
    "    :param bed_paths:\n",
    "    :param df_info:\n",
    "    :param max_distance:\n",
    "    :param resolution:\n",
    "    :param stride:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df_train = pd.concat([extract_features(path,df_info, max_distance, resolution, stride) for path in bed_paths], axis=1)\n",
    "    df_train.columns = [i for i in range(df_train.columns.size)]\n",
    "\n",
    "    return df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me1-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K9me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27ac-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K36me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me1-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K9me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27ac-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K36me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me1-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K9me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27ac-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K36me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "### PREPARE DATA FOR TRAINING\n",
    "# set preprocessing parameters\n",
    "max_distance = 50000\n",
    "resolution = 250\n",
    "stride = 125\n",
    "\n",
    "# load labels\n",
    "train_y_X1 = pd.read_csv(train_y_X1_path, delimiter=\"\\t\")\n",
    "train_x_X1 = create_set(bed_file_paths_X1, train_info_X1_path, max_distance, resolution, stride)\n",
    "\n",
    "val_y_X1 = pd.read_csv(val_y_X1_path, delimiter=\"\\t\")\n",
    "val_x_X1 = create_set(bed_file_paths_X1, val_info_X1_path, max_distance, resolution, stride)\n",
    "\n",
    "val_y_X2 = pd.read_csv(val_y_X2_path, delimiter=\"\\t\")\n",
    "val_x_X2 = create_set(bed_file_paths_X2, val_info_X2_path, max_distance, resolution, stride)\n",
    "\n",
    "\n",
    "# store datasets\n",
    "train_x_X1.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/train_x_X1_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)\n",
    "val_x_X1.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X1_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)\n",
    "val_x_X2.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X2_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "### LOAD DATASETS\n",
    "max_distance = 2500\n",
    "resolution = 250\n",
    "stride = 125\n",
    "\n",
    "train_y_X1 = pd.read_csv(train_y_X1_path, delimiter=\"\\t\")\n",
    "train_x_X1 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/train_x_X1_\"+\n",
    "                         str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n",
    "\n",
    "val_y_X1 = pd.read_csv(val_y_X1_path, delimiter=\"\\t\")\n",
    "val_x_X1 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X1_\"+\n",
    "                       str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n",
    "\n",
    "val_y_X2 = pd.read_csv(val_y_X2_path, delimiter=\"\\t\")\n",
    "val_x_X2 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X2_\"+\n",
    "                       str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work Package 1.2 - Model Building"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-rmse:294.74146\ttrain-rmse:295.17955\n",
      "RMSE of the base model: 270.334\n",
      "SPMC of the base model: 0.637\n"
     ]
    }
   ],
   "source": [
    "### XGBOOST MODEL\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create regression matrices\n",
    "dtrain_reg = xgb.DMatrix(train_x_X1, train_y_X1[\"gex\"], enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(val_x_X1, val_y_X1[\"gex\"], enable_categorical=True)\n",
    "\n",
    "# Define training parameters\n",
    "params = {\"objective\": \"reg:squarederror\"}\n",
    "evals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n",
    "n = 1\n",
    "\n",
    "# Train xgboost model\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain_reg,\n",
    "    num_boost_round=n,\n",
    "    evals=evals,\n",
    "    verbose_eval=1,\n",
    "    # Activate early stopping\n",
    "    early_stopping_rounds=1\n",
    ")\n",
    "\n",
    "# Make predictions and score\n",
    "preds = model.predict(dtest_reg)\n",
    "rmse = mean_squared_error(val_y_X2[\"gex\"], preds, squared=False)\n",
    "spmc = stats.spearmanr(preds, val_y_X2[\"gex\"]).statistic\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "print(f\"SPMC of the base model: {spmc:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/30 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa4ef34684ca4e1e8ebcda86dc8d25d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -97806.78674833744\n",
      "\n",
      "Generation 2 - Current best internal CV score: -97806.78674833744\n",
      "\n",
      "Generation 3 - Current best internal CV score: -97806.78674833744\n",
      "\n",
      "Generation 4 - Current best internal CV score: -97483.61103847323\n",
      "\n",
      "Generation 5 - Current best internal CV score: -97400.56653143391\n",
      "\n",
      "Best pipeline: RandomForestRegressor(LassoLarsCV(LinearSVR(input_matrix, C=5.0, dual=True, epsilon=0.0001, loss=epsilon_insensitive, tol=0.01), normalize=True), bootstrap=True, max_features=0.05, min_samples_leaf=7, min_samples_split=13, n_estimators=100)\n"
     ]
    }
   ],
   "source": [
    "### TEAPOT ANALYSIS\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "#Run teapot analysis\n",
    "version = \"1.1\"\n",
    "tpot = TPOTRegressor(generations=5, population_size=5, verbosity=2, random_state=42)\n",
    "tpot.fit(train_x_X1.to_numpy(), train_y_X1[\"gex\"].to_numpy())\n",
    "tpot.export('teapots/teapot_'+version+'.py')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the base model: 259.884\n",
      "SPMC of the base model: 0.638\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "preds = tpot.predict(val_x_X2.to_numpy())\n",
    "rmse = mean_squared_error(val_y_X2[\"gex\"], preds, squared=False)\n",
    "spmc = stats.spearmanr(preds, val_y_X2[\"gex\"]).statistic\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "print(f\"SPMC of the base model: {spmc:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Using the model trained in WP 1.2, make predictions on the test data (chr 1 of cell line X3).\n",
    "# Store predictions in a variable called \"pred\" which is a numpy array.\n",
    "\n",
    "pred = None\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Check if \"pred\" meets the specified constrains\n",
    "assert isinstance(pred, np.ndarray), 'Prediction array must be a numpy array'\n",
    "assert np.issubdtype(pred.dtype, np.number), 'Prediction array must be numeric'\n",
    "assert pred.shape[0] == len(test_genes), 'Each gene should have a unique predicted expression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = 'path/to/save/output/file'  # TODO\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"LastName_FirstName_Project1.zip\" # TODO\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "test_genes['gex_predicted'] = pred.tolist()\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
