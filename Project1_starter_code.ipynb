{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that are required to run your project\n",
    "# You are allowed to add more libraries as you need\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PATHS\n",
    "\n",
    "# NOTE: \n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you. \n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "# Path for datasets\n",
    "path_cwd = os.getcwd()\n",
    "path_data = path_cwd+\"/ML4G_Project_1_Data\"\n",
    "\n",
    "# Metadata for genes of cell lines X1 and X2\n",
    "train_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_info.tsv\"\n",
    "train_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_info.tsv\"\n",
    "val_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_info.tsv\"\n",
    "val_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_info.tsv\"\n",
    "\n",
    "# Gene expression values for cell lines X1 and X2\n",
    "train_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_y.tsv\"\n",
    "train_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_y.tsv\"\n",
    "val_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_y.tsv\"\n",
    "val_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_y.tsv\"\n",
    "\n",
    "# DNase and histone modification data for cell lines X1, X2 and X3\n",
    "bed_files_X1 = [\"/DNase-bed/X1.bed\",\n",
    "                \"/H3K4me1-bed/X1.bed\",\n",
    "                \"/H3K4me3-bed/X1.bed\",\n",
    "                \"/H3K9me3-bed/X1.bed\",\n",
    "                \"/H3K27ac-bed/X1.bed\",\n",
    "                \"/H3K27me3-bed/X1.bed\",\n",
    "                \"/H3K36me3-bed/X1.bed\"]\n",
    "bed_file_paths_X1 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X2 = [\"/DNase-bed/X2.bed\",\n",
    "                \"/H3K4me1-bed/X2.bed\",\n",
    "                \"/H3K4me3-bed/X2.bed\",\n",
    "                \"/H3K9me3-bed/X2.bed\",\n",
    "                \"/H3K27ac-bed/X2.bed\",\n",
    "                \"/H3K27me3-bed/X2.bed\",\n",
    "                \"/H3K36me3-bed/X2.bed\"]\n",
    "bed_file_paths_X2 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X3 = [\"/DNase-bed/X3.bed\",\n",
    "                \"/H3K4me1-bed/X3.bed\",\n",
    "                \"/H3K4me3-bed/X3.bed\",\n",
    "                \"/H3K9me3-bed/X3.bed\",\n",
    "                \"/H3K27ac-bed/X3.bed\",\n",
    "                \"/H3K27me3-bed/X3.bed\",\n",
    "                \"/H3K36me3-bed/X3.bed\"]\n",
    "bed_file_paths_X3 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "# Small dataset for debugging\n",
    "debug_info_path = path_data+\"/info.tsv\"\n",
    "debug_bed_file = path_data+\"/bed_file.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "df_peak_data = pd.read_csv(bed_file_paths_X1[0], sep='\\t')#, usecols=[0,1,2,4], names = [\"chromosome\", \"peak_start\", \"peak_end\", \"score\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "       chr1    181405    181555  .  0 ..1  12.379542683931337  -1  -1.1  75\n0      chr1    267990    268140  .  0   .           12.379543  -1    -1  75\n1      chr1    629160    629310  .  0   .           22.263896  -1    -1  75\n2      chr1    629494    629644  .  0   .           10.732151  -1    -1  75\n3      chr1    629870    630020  .  0   .         3603.694491  -1    -1  75\n4      chr1    630155    630305  .  0   .           15.674327  -1    -1  75\n...     ...       ...       ... .. ..  ..                 ...  ..   ...  ..\n56446  chrY  10627255  10627405  .  0   .           68.390877  -1    -1  75\n56447  chrY  10994543  10994693  .  0   .            8.261062  -1    -1  75\n56448  chrY  11302695  11302845  .  0   .           34.619337  -1    -1  75\n56449  chrY  11329834  11329984  .  0   .           25.558680  -1    -1  75\n56450  chrY  15355495  15355645  .  0   .           17.321719  -1    -1  75\n\n[56451 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chr1</th>\n      <th>181405</th>\n      <th>181555</th>\n      <th>.</th>\n      <th>0</th>\n      <th>..1</th>\n      <th>12.379542683931337</th>\n      <th>-1</th>\n      <th>-1.1</th>\n      <th>75</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>chr1</td>\n      <td>267990</td>\n      <td>268140</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>12.379543</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>chr1</td>\n      <td>629160</td>\n      <td>629310</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>22.263896</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>chr1</td>\n      <td>629494</td>\n      <td>629644</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>10.732151</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chr1</td>\n      <td>629870</td>\n      <td>630020</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>3603.694491</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>chr1</td>\n      <td>630155</td>\n      <td>630305</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>15.674327</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>56446</th>\n      <td>chrY</td>\n      <td>10627255</td>\n      <td>10627405</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>68.390877</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>56447</th>\n      <td>chrY</td>\n      <td>10994543</td>\n      <td>10994693</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>8.261062</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>56448</th>\n      <td>chrY</td>\n      <td>11302695</td>\n      <td>11302845</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>34.619337</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>56449</th>\n      <td>chrY</td>\n      <td>11329834</td>\n      <td>11329984</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>25.558680</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>56450</th>\n      <td>chrY</td>\n      <td>15355495</td>\n      <td>15355645</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>17.321719</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n  </tbody>\n</table>\n<p>56451 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_peak_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "### FUNCTION FOR EXTRACTION OF FEATURES\n",
    "def extract_features(bed_path, info_path, max_distance, resolution, stride, use_score=True):\n",
    "    \"\"\"\n",
    "    Function extracting binary features from bed datasets\n",
    "    :param bed_path: path to bed file of interest\n",
    "    :param info_path: path to info file of interest\n",
    "    :param max_distance: maximal distance from TSS that should be considered\n",
    "    :param resolution: window size of aggregation for dimensionality reduction\n",
    "    :param stride: stride for dimensionality reduction\n",
    "    :return: pandas df of type int8 containing binary features\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df_info = pd.read_csv(info_path, sep='\\t', usecols=[0,1,4])\n",
    "\n",
    "    if (\"DNase\" in bed_path):\n",
    "        score_col = 6\n",
    "    else: score_col = 4\n",
    "\n",
    "    df_peak_data = pd.read_csv(bed_path, sep='\\t', usecols=[0,1,2,score_col], names = [\"chromosome\", \"peak_start\", \"peak_end\", \"score\"])\n",
    "\n",
    "    # Get genes and initialize features df with False as entries\n",
    "    df_features = pd.DataFrame(data=0,columns=[i-max_distance-1 for i in range(1, 2*(max_distance+1))], index=df_info[\"gene_name\"], dtype=\"int8\")\n",
    "\n",
    "    # Fill df according to info data\n",
    "    for i in df_info.index:\n",
    "        if i > 5: break\n",
    "        gene = df_info[\"gene_name\"][i]\n",
    "        tss = df_info[\"TSS_start\"][i]\n",
    "        chromosome = df_info[\"chr\"][i]\n",
    "        tss_l = tss - max_distance\n",
    "        tss_r = tss + max_distance\n",
    "\n",
    "        # Print progress\n",
    "        if i == 0:\n",
    "            print(\"Start preprocessing of:\", \"\\n\"+\n",
    "                  \"Dataset:\", bed_path, \"\\n\"+\n",
    "                  \"Infoset:\", info_path)\n",
    "        if i == df_info.index[-1]:\n",
    "            print(\"Done!\" + \"\\n\" + \"-----------------------------------\")\n",
    "\n",
    "        # Find relevant peaks\n",
    "        peaks = df_peak_data.loc[(df_peak_data[\"peak_start\"] <= tss_r) &\n",
    "                                 (df_peak_data[\"peak_end\"] >= tss_l)]\n",
    "\n",
    "        # Fill features dataset\n",
    "        for j in range(peaks.shape[0]):\n",
    "            # Make sure that peak is on the same chromosome\n",
    "            if peaks[\"chromosome\"].iloc[j] != chromosome: continue\n",
    "\n",
    "            # Get peak boundaries\n",
    "            peak_l = peaks[\"peak_start\"].iloc[j]\n",
    "            peak_r = peaks[\"peak_end\"].iloc[j]\n",
    "\n",
    "            if use_score:\n",
    "                # Get score\n",
    "                score = peaks[\"score\"].iloc[j]\n",
    "            else:\n",
    "                score = 1\n",
    "\n",
    "            # Consider possible cases\n",
    "            if (peak_l >= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : peak_r-tss] = score\n",
    "\n",
    "            elif (peak_l <= tss_r) and (peak_r >= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : tss_r-tss] = score\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : peak_r-tss] = score\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r >= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : tss_r-tss] = score\n",
    "\n",
    "    # Introduce resolution (rather inefficient...)\n",
    "    df_features=df_features.rolling(window=resolution,\n",
    "                                      axis=1,\n",
    "                                      step=stride,\n",
    "                                      min_periods=1,\n",
    "                                      center=True).mean()\n",
    "\n",
    "    return df_features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[57], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbed_file_paths_X1\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_info_X1_path\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43muse_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[56], line 69\u001B[0m, in \u001B[0;36mextract_features\u001B[0;34m(bed_path, info_path, max_distance, resolution, stride, use_score)\u001B[0m\n\u001B[1;32m     66\u001B[0m     df_features\u001B[38;5;241m.\u001B[39mloc[[gene], peak_l\u001B[38;5;241m-\u001B[39mtss : tss_r\u001B[38;5;241m-\u001B[39mtss] \u001B[38;5;241m=\u001B[39m score\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m (peak_l \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m tss_l) \u001B[38;5;129;01mand\u001B[39;00m (peak_r \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m tss_r):\n\u001B[0;32m---> 69\u001B[0m     df_features\u001B[38;5;241m.\u001B[39mloc[[gene], tss_l\u001B[38;5;241m-\u001B[39mtss : peak_r\u001B[38;5;241m-\u001B[39mtss] \u001B[38;5;241m=\u001B[39m score\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m (peak_l \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m tss_l) \u001B[38;5;129;01mand\u001B[39;00m (peak_r \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m tss_r):\n\u001B[1;32m     72\u001B[0m     df_features\u001B[38;5;241m.\u001B[39mloc[[gene], tss_l\u001B[38;5;241m-\u001B[39mtss : tss_r\u001B[38;5;241m-\u001B[39mtss] \u001B[38;5;241m=\u001B[39m score\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/indexing.py:818\u001B[0m, in \u001B[0;36m_LocationIndexer.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m    815\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_valid_setitem_indexer(key)\n\u001B[1;32m    817\u001B[0m iloc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miloc\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39miloc\n\u001B[0;32m--> 818\u001B[0m \u001B[43miloc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setitem_with_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/indexing.py:1795\u001B[0m, in \u001B[0;36m_iLocIndexer._setitem_with_indexer\u001B[0;34m(self, indexer, value, name)\u001B[0m\n\u001B[1;32m   1792\u001B[0m \u001B[38;5;66;03m# align and set the values\u001B[39;00m\n\u001B[1;32m   1793\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m take_split_path:\n\u001B[1;32m   1794\u001B[0m     \u001B[38;5;66;03m# We have to operate column-wise\u001B[39;00m\n\u001B[0;32m-> 1795\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setitem_with_indexer_split_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1796\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1797\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/indexing.py:1888\u001B[0m, in \u001B[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001B[0;34m(self, indexer, value, name)\u001B[0m\n\u001B[1;32m   1884\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1885\u001B[0m \n\u001B[1;32m   1886\u001B[0m     \u001B[38;5;66;03m# scalar value\u001B[39;00m\n\u001B[1;32m   1887\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m loc \u001B[38;5;129;01min\u001B[39;00m ilocs:\n\u001B[0;32m-> 1888\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setitem_single_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpi\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/indexing.py:1992\u001B[0m, in \u001B[0;36m_iLocIndexer._setitem_single_column\u001B[0;34m(self, loc, value, plane_indexer)\u001B[0m\n\u001B[1;32m   1988\u001B[0m         value \u001B[38;5;241m=\u001B[39m value[pi]\n\u001B[1;32m   1989\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1990\u001B[0m     \u001B[38;5;66;03m# set value into the column (first attempting to operate inplace, then\u001B[39;00m\n\u001B[1;32m   1991\u001B[0m     \u001B[38;5;66;03m#  falling back to casting if necessary)\u001B[39;00m\n\u001B[0;32m-> 1992\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_setitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplane_indexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1993\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_clear_item_cache()\n\u001B[1;32m   1994\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/internals/managers.py:1387\u001B[0m, in \u001B[0;36mBlockManager.column_setitem\u001B[0;34m(self, loc, idx, value, inplace)\u001B[0m\n\u001B[1;32m   1383\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clear_reference_block(blkno)\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;66;03m# this manager is only created temporarily to mutate the values in place\u001B[39;00m\n\u001B[1;32m   1386\u001B[0m \u001B[38;5;66;03m# so don't track references, otherwise the `setitem` would perform CoW again\u001B[39;00m\n\u001B[0;32m-> 1387\u001B[0m col_mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrack_ref\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[1;32m   1389\u001B[0m     col_mgr\u001B[38;5;241m.\u001B[39msetitem_inplace(idx, value)\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/internals/managers.py:1129\u001B[0m, in \u001B[0;36mBlockManager.iget\u001B[0;34m(self, i, track_ref)\u001B[0m\n\u001B[1;32m   1126\u001B[0m     block \u001B[38;5;241m=\u001B[39m new_block(result, placement\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mslice\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(result)), ndim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   1127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SingleBlockManager(block, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m-> 1129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21miget\u001B[39m(\u001B[38;5;28mself\u001B[39m, i: \u001B[38;5;28mint\u001B[39m, track_ref: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SingleBlockManager:\n\u001B[1;32m   1130\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;124;03m    Return the data as a SingleBlockManager.\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1133\u001B[0m     block \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblknos[i]]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "extract_features(bed_file_paths_X1[0], train_info_X1_path,100,1,1,use_score=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "### FUNCTION FOR CREATING TRAINING DATASET\n",
    "def create_set(bed_paths, df_info, max_distance, resolution, stride, use_score=True):\n",
    "    \"\"\"\n",
    "    Create training dataset\n",
    "    :param bed_paths:\n",
    "    :param df_info:\n",
    "    :param max_distance:\n",
    "    :param resolution:\n",
    "    :param stride:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    df_train = pd.concat([extract_features(path,df_info, max_distance, resolution, stride, use_score) for path in bed_paths], axis=1)\n",
    "    df_train.columns = [i for i in range(df_train.columns.size)]\n",
    "\n",
    "    return df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me1-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K9me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27ac-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K36me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me1-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K9me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27ac-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K36me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me1-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K9me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27ac-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K36me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n"
     ]
    }
   ],
   "source": [
    "### PREPARE DATA FOR TRAINING\n",
    "# set preprocessing parameter\n",
    "# s\n",
    "\n",
    "max_distance = 500\n",
    "resolution = 20\n",
    "stride = 10\n",
    "\n",
    "# load labels\n",
    "train_y_X1 = pd.read_csv(train_y_X1_path, delimiter=\"\\t\")\n",
    "train_x_X1 = create_set(bed_file_paths_X1, train_info_X1_path, max_distance, resolution, stride, use_score=False)\n",
    "\n",
    "val_y_X1 = pd.read_csv(val_y_X1_path, delimiter=\"\\t\")\n",
    "val_x_X1 = create_set(bed_file_paths_X1, val_info_X1_path, max_distance, resolution, stride, use_score=False)\n",
    "\n",
    "val_y_X2 = pd.read_csv(val_y_X2_path, delimiter=\"\\t\")\n",
    "val_x_X2 = create_set(bed_file_paths_X2, val_info_X2_path, max_distance, resolution, stride, use_score=False)\n",
    "\n",
    "# store datasets\n",
    "# train_x_X1.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/train_x_X1_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)\n",
    "# val_x_X1.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X1_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)\n",
    "# val_x_X2.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X2_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "### LOAD DATASETS\n",
    "max_distance = 10000\n",
    "resolution = 500\n",
    "stride = 250\n",
    "\n",
    "train_y_X1 = pd.read_csv(train_y_X1_path, delimiter=\"\\t\")\n",
    "train_x_X1 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/train_x_X1_\"+\n",
    "                         str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n",
    "\n",
    "val_y_X1 = pd.read_csv(val_y_X1_path, delimiter=\"\\t\")\n",
    "val_x_X1 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X1_\"+\n",
    "                       str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n",
    "\n",
    "val_y_X2 = pd.read_csv(val_y_X2_path, delimiter=\"\\t\")\n",
    "val_x_X2 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X2_\"+\n",
    "                       str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work Package 1.2 - Model Building"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-map:0.88516\ttrain-map:0.87375\n",
      "[1]\tvalidation-map:0.89378\ttrain-map:0.88973\n",
      "[2]\tvalidation-map:0.88978\ttrain-map:0.89156\n",
      "[3]\tvalidation-map:0.89031\ttrain-map:0.89460\n",
      "[4]\tvalidation-map:0.89458\ttrain-map:0.89723\n",
      "[5]\tvalidation-map:0.89492\ttrain-map:0.90082\n",
      "[6]\tvalidation-map:0.89464\ttrain-map:0.90186\n",
      "[7]\tvalidation-map:0.89538\ttrain-map:0.90189\n",
      "[8]\tvalidation-map:0.89707\ttrain-map:0.90368\n",
      "[9]\tvalidation-map:0.89867\ttrain-map:0.90543\n",
      "[10]\tvalidation-map:0.89883\ttrain-map:0.90675\n",
      "RMSE of the base model: 276.650\n",
      "SPMC of the base model: 0.698\n"
     ]
    }
   ],
   "source": [
    "### XGBOOST MODEL\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create regression matrices\n",
    "dtrain_reg = xgb.DMatrix(train_x_X1, train_y_X1[\"gex\"], enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(val_x_X1, val_y_X1[\"gex\"], enable_categorical=True)\n",
    "\n",
    "# Define training parameters\n",
    "params = {\"objective\": \"rank:pairwise\"}\n",
    "evals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n",
    "n = 11\n",
    "\n",
    "# Train xgboost model\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain_reg,\n",
    "    num_boost_round=n,\n",
    "    evals=evals,\n",
    "    verbose_eval=1,\n",
    "    # Activate early stopping\n",
    "    early_stopping_rounds=1\n",
    ")\n",
    "\n",
    "# Make predictions and score\n",
    "dtest_reg = xgb.DMatrix(val_x_X2, val_y_X2[\"gex\"], enable_categorical=True)\n",
    "preds = model.predict(dtest_reg)\n",
    "rmse = mean_squared_error(val_y_X2[\"gex\"], preds, squared=False)\n",
    "spmc = stats.spearmanr(preds, val_y_X2[\"gex\"]).statistic\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "print(f\"SPMC of the base model: {spmc:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "\n",
    "def score_func(y, y_pred):\n",
    "    return spearmanr(y,y_pred).statistic\n",
    "scorer=make_scorer(score_func)\n",
    "\n",
    "dtrain_reg = xgb.DMatrix(train_x_X1, train_y_X1[\"gex\"], enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(val_x_X1, val_y_X1[\"gex\"], enable_categorical=True)\n",
    "\n",
    "model=xgb.XGBRegressor(booster='gbtree')\n",
    "param_grid={\n",
    "    'n_estimators': Integer(60,200),\n",
    "    'learning_rate': Real(1e-5,1e-1)\n",
    "\n",
    "                          ,prior='log-uniform'),\n",
    "    'max_depth': Integer(1,10),\n",
    "}\n",
    "\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=scorer,\n",
    "    n_iter=100,\n",
    "    random_state=7,\n",
    "    cv=5,\n",
    "    verbose=3)\n",
    "\n",
    "opt.fit(train_x_X1, train_y_X1[\"gex\"])\n",
    "\n",
    "print(f'Best CV on same Cell Line Score: {opt.best_score_}')\n",
    "print(f'Best params: {opt.best_params_}')\n",
    "y_hat_2= opt.predict(val_x_X2)\n",
    "score=spearmanr(y_hat_2, val_y_X2)\n",
    "print(f'Score on different Cell Line: {score}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_hat_2= opt.predict(val_x_X2)\n",
    "score=spearmanr(y_hat_2, val_y_X2['gex'])\n",
    "print(f'Score on different Cell Line: {score.statistic}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_hat_2.shape\n",
    "val_y_X2.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/30 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f1e376f4da343ee897e7d62d1e9a5f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m version \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m tpot \u001B[38;5;241m=\u001B[39m TPOTRegressor(generations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, population_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, verbosity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mtpot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x_X1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_y_X1\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgex\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m tpot\u001B[38;5;241m.\u001B[39mexport(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mteapots/teapot_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39mversion\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.py\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/tpot/base.py:863\u001B[0m, in \u001B[0;36mTPOTBase.fit\u001B[0;34m(self, features, target, sample_weight, groups)\u001B[0m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mSystemExit\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    861\u001B[0m         \u001B[38;5;66;03m# raise the exception if it's our last attempt\u001B[39;00m\n\u001B[1;32m    862\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m attempt \u001B[38;5;241m==\u001B[39m (attempts \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m--> 863\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    864\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/tpot/base.py:854\u001B[0m, in \u001B[0;36mTPOTBase.fit\u001B[0;34m(self, features, target, sample_weight, groups)\u001B[0m\n\u001B[1;32m    851\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pbar, \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[1;32m    852\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pbar\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m--> 854\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_top_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_summary_of_best_pipeline(features, target)\n\u001B[1;32m    856\u001B[0m \u001B[38;5;66;03m# Delete the temporary cache before exiting\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/tpot/base.py:961\u001B[0m, in \u001B[0;36mTPOTBase._update_top_pipeline\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    957\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_optimized_pareto_front_n_gens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    958\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    959\u001B[0m     \u001B[38;5;66;03m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001B[39;00m\n\u001B[1;32m    960\u001B[0m     \u001B[38;5;66;03m# need raise RuntimeError because no pipeline has been optimized\u001B[39;00m\n\u001B[0;32m--> 961\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    962\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA pipeline has not yet been optimized. Please call fit() first.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    963\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "### TEAPOT ANALYSIS\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "#Run teapot analysis\n",
    "version = \"1.2\"\n",
    "tpot = TPOTRegressor(generations=5, population_size=5, verbosity=2, random_state=42)\n",
    "tpot.fit(train_x_X1.to_numpy(), train_y_X1[\"gex\"].to_numpy())\n",
    "tpot.export('teapots/teapot_'+version+'.py')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "preds = tpot.predict(val_x_X2.to_numpy())\n",
    "rmse = mean_squared_error(val_y_X2[\"gex\"], preds, squared=False)\n",
    "spmc = stats.spearmanr(preds, val_y_X2[\"gex\"]).statistic\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "print(f\"SPMC of the base model: {spmc:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Using the model trained in WP 1.2, make predictions on the test data (chr 1 of cell line X3).\n",
    "# Store predictions in a variable called \"pred\" which is a numpy array.\n",
    "\n",
    "pred = None\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Check if \"pred\" meets the specified constrains\n",
    "assert isinstance(pred, np.ndarray), 'Prediction array must be a numpy array'\n",
    "assert np.issubdtype(pred.dtype, np.number), 'Prediction array must be numeric'\n",
    "assert pred.shape[0] == len(test_genes), 'Each gene should have a unique predicted expression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = 'path/to/save/output/file'  # TODO\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"LastName_FirstName_Project1.zip\" # TODO\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "test_genes['gex_predicted'] = pred.tolist()\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
