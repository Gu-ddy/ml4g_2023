{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that are required to run your project\n",
    "# You are allowed to add more libraries as you need\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PATHS\n",
    "\n",
    "# NOTE: \n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you. \n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "# Path for datasets\n",
    "path_cwd = os.getcwd()\n",
    "path_data = path_cwd+\"/ML4G_Project_1_Data\"\n",
    "\n",
    "# Metadata for genes of cell lines X1 and X2\n",
    "train_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_info.tsv\"\n",
    "train_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_info.tsv\"\n",
    "val_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_info.tsv\"\n",
    "val_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_info.tsv\"\n",
    "\n",
    "# Gene expression values for cell lines X1 and X2\n",
    "train_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_y.tsv\"\n",
    "train_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_y.tsv\"\n",
    "val_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_y.tsv\"\n",
    "val_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_y.tsv\"\n",
    "\n",
    "# DNase and histone modification data for cell lines X1, X2 and X3\n",
    "bed_files_X1 = [\"/DNase-bed/X1.bed\",\n",
    "                \"/H3K4me1-bed/X1.bed\",\n",
    "                \"/H3K4me3-bed/X1.bed\",\n",
    "                \"/H3K9me3-bed/X1.bed\",\n",
    "                \"/H3K27ac-bed/X1.bed\",\n",
    "                \"/H3K27me3-bed/X1.bed\",\n",
    "                \"/H3K36me3-bed/X1.bed\"]\n",
    "bed_file_paths_X1 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X2 = [\"/DNase-bed/X2.bed\",\n",
    "                \"/H3K4me1-bed/X2.bed\",\n",
    "                \"/H3K4me3-bed/X2.bed\",\n",
    "                \"/H3K9me3-bed/X2.bed\",\n",
    "                \"/H3K27ac-bed/X2.bed\",\n",
    "                \"/H3K27me3-bed/X2.bed\",\n",
    "                \"/H3K36me3-bed/X2.bed\"]\n",
    "bed_file_paths_X2 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X3 = [\"/DNase-bed/X3.bed\",\n",
    "                \"/H3K4me1-bed/X3.bed\",\n",
    "                \"/H3K4me3-bed/X3.bed\",\n",
    "                \"/H3K9me3-bed/X3.bed\",\n",
    "                \"/H3K27ac-bed/X3.bed\",\n",
    "                \"/H3K27me3-bed/X3.bed\",\n",
    "                \"/H3K36me3-bed/X3.bed\"]\n",
    "bed_file_paths_X3 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "# Small dataset for debugging\n",
    "debug_info_path = path_data+\"/info.tsv\"\n",
    "debug_bed_file = path_data+\"/bed_file.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "df_peak_data = pd.read_csv(bed_file_paths_X1[0], sep='\\t')#, usecols=[0,1,2,4], names = [\"chromosome\", \"peak_start\", \"peak_end\", \"score\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "       chr1    181405    181555  .  0 ..1  12.379542683931337  -1  -1.1  75\n0      chr1    267990    268140  .  0   .           12.379543  -1    -1  75\n1      chr1    629160    629310  .  0   .           22.263896  -1    -1  75\n2      chr1    629494    629644  .  0   .           10.732151  -1    -1  75\n3      chr1    629870    630020  .  0   .         3603.694491  -1    -1  75\n4      chr1    630155    630305  .  0   .           15.674327  -1    -1  75\n...     ...       ...       ... .. ..  ..                 ...  ..   ...  ..\n56446  chrY  10627255  10627405  .  0   .           68.390877  -1    -1  75\n56447  chrY  10994543  10994693  .  0   .            8.261062  -1    -1  75\n56448  chrY  11302695  11302845  .  0   .           34.619337  -1    -1  75\n56449  chrY  11329834  11329984  .  0   .           25.558680  -1    -1  75\n56450  chrY  15355495  15355645  .  0   .           17.321719  -1    -1  75\n\n[56451 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chr1</th>\n      <th>181405</th>\n      <th>181555</th>\n      <th>.</th>\n      <th>0</th>\n      <th>..1</th>\n      <th>12.379542683931337</th>\n      <th>-1</th>\n      <th>-1.1</th>\n      <th>75</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>chr1</td>\n      <td>267990</td>\n      <td>268140</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>12.379543</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>chr1</td>\n      <td>629160</td>\n      <td>629310</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>22.263896</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>chr1</td>\n      <td>629494</td>\n      <td>629644</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>10.732151</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chr1</td>\n      <td>629870</td>\n      <td>630020</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>3603.694491</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>chr1</td>\n      <td>630155</td>\n      <td>630305</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>15.674327</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>56446</th>\n      <td>chrY</td>\n      <td>10627255</td>\n      <td>10627405</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>68.390877</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>56447</th>\n      <td>chrY</td>\n      <td>10994543</td>\n      <td>10994693</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>8.261062</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>56448</th>\n      <td>chrY</td>\n      <td>11302695</td>\n      <td>11302845</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>34.619337</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>56449</th>\n      <td>chrY</td>\n      <td>11329834</td>\n      <td>11329984</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>25.558680</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>56450</th>\n      <td>chrY</td>\n      <td>15355495</td>\n      <td>15355645</td>\n      <td>.</td>\n      <td>0</td>\n      <td>.</td>\n      <td>17.321719</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>75</td>\n    </tr>\n  </tbody>\n</table>\n<p>56451 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_peak_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "### FUNCTION FOR EXTRACTION OF FEATURES\n",
    "def extract_features(bed_path, info_path, max_distance, resolution, stride):\n",
    "    \"\"\"\n",
    "    Function extracting binary features from bed datasets\n",
    "    :param bed_path: path to bed file of interest\n",
    "    :param info_path: path to info file of interest\n",
    "    :param max_distance: maximal distance from TSS that should be considered\n",
    "    :param resolution: window size of aggregation for dimensionality reduction\n",
    "    :param stride: stride for dimensionality reduction\n",
    "    :return: pandas df of type int8 containing binary features\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df_info = pd.read_csv(info_path, sep='\\t', usecols=[0,1,4])\n",
    "\n",
    "    if (\"DNase\" in bed_path):\n",
    "        score_col = 6\n",
    "    else: score_col = 4\n",
    "\n",
    "    df_peak_data = pd.read_csv(bed_path, sep='\\t', usecols=[0,1,2,score_col], names = [\"chromosome\", \"peak_start\", \"peak_end\", \"score\"])\n",
    "\n",
    "    # Get genes and initialize features df with False as entries\n",
    "    df_features = pd.DataFrame(data=0,columns=[i-max_distance-1 for i in range(1, 2*(max_distance+1))], index=df_info[\"gene_name\"], dtype=\"int8\")\n",
    "\n",
    "    # Fill df according to info data\n",
    "    for i in df_info.index:\n",
    "\n",
    "        gene = df_info[\"gene_name\"][i]\n",
    "        tss = df_info[\"TSS_start\"][i]\n",
    "        chromosome = df_info[\"chr\"][i]\n",
    "        tss_l = tss - max_distance\n",
    "        tss_r = tss + max_distance\n",
    "\n",
    "        # Print progress\n",
    "        if i == 0:\n",
    "            print(\"Start preprocessing of:\", \"\\n\"+\n",
    "                  \"Dataset:\", bed_path, \"\\n\"+\n",
    "                  \"Infoset:\", info_path)\n",
    "        if i == df_info.index[-1]:\n",
    "            print(\"Done!\" + \"\\n\" + \"-----------------------------------\")\n",
    "\n",
    "        # Find relevant peaks\n",
    "        peaks = df_peak_data.loc[(df_peak_data[\"peak_start\"] <= tss_r) &\n",
    "                                 (df_peak_data[\"peak_end\"] >= tss_l)]\n",
    "\n",
    "        # Fill features dataset\n",
    "        for j in range(peaks.shape[0]):\n",
    "            # Make sure that peak is on the same chromosome\n",
    "            if peaks[\"chromosome\"].iloc[j] != chromosome: continue\n",
    "\n",
    "            # Get peak boundaries\n",
    "            peak_l = peaks[\"peak_start\"].iloc[j]\n",
    "            peak_r = peaks[\"peak_end\"].iloc[j]\n",
    "\n",
    "            if use_score:\n",
    "                # Get score\n",
    "                score = peaks[\"score\"].iloc[j]\n",
    "            else:\n",
    "                score = 1\n",
    "\n",
    "            # Consider possible cases\n",
    "            if (peak_l >= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : peak_r-tss] = score\n",
    "\n",
    "            elif (peak_l <= tss_r) and (peak_r >= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : tss_r-tss] = score\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : peak_r-tss] = score\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r >= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : tss_r-tss] = score\n",
    "\n",
    "    # Introduce resolution (rather inefficient...)\n",
    "    df_features=df_features.rolling(window=resolution,\n",
    "                                      axis=1,\n",
    "                                      step=stride,\n",
    "                                      min_periods=1,\n",
    "                                      center=True).mean()\n",
    "\n",
    "    # df_features[df_features >= 0.5] = 1\n",
    "    # df_features[df_features < 0.5] = 0\n",
    "\n",
    "    return df_features#.astype(\"int8\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "107.92828901848374\n",
      "59.33021976211656\n",
      "45.3273862475701\n",
      "38.73781753484234\n"
     ]
    },
    {
     "data": {
      "text/plain": "                 -100        -99         -98         -97         -96   \\\ngene_name                                                               \nSLC20A1      0.000000    0.000000    0.000000    0.000000    0.000000   \nC11orf58   107.928289  107.928289  107.928289  107.928289  107.928289   \nZSCAN9       0.000000    0.000000    0.000000    0.000000    0.000000   \nCD19         0.000000    0.000000    0.000000    0.000000    0.000000   \nTMEM123     38.737818   38.737818   38.737818   38.737818   38.737818   \n...               ...         ...         ...         ...         ...   \nACOX1        0.000000    0.000000    0.000000    0.000000    0.000000   \nMLXIP        0.000000    0.000000    0.000000    0.000000    0.000000   \nASGR2        0.000000    0.000000    0.000000    0.000000    0.000000   \nOR5A1        0.000000    0.000000    0.000000    0.000000    0.000000   \nTLR3         0.000000    0.000000    0.000000    0.000000    0.000000   \n\n                 -95         -94         -93         -92         -91   ...  \\\ngene_name                                                              ...   \nSLC20A1      0.000000    0.000000    0.000000    0.000000    0.000000  ...   \nC11orf58   107.928289  107.928289  107.928289  107.928289  107.928289  ...   \nZSCAN9       0.000000    0.000000    0.000000    0.000000    0.000000  ...   \nCD19         0.000000    0.000000    0.000000    0.000000    0.000000  ...   \nTMEM123     38.737818   38.737818   38.737818   38.737818   38.737818  ...   \n...               ...         ...         ...         ...         ...  ...   \nACOX1        0.000000    0.000000    0.000000    0.000000    0.000000  ...   \nMLXIP        0.000000    0.000000    0.000000    0.000000    0.000000  ...   \nASGR2        0.000000    0.000000    0.000000    0.000000    0.000000  ...   \nOR5A1        0.000000    0.000000    0.000000    0.000000    0.000000  ...   \nTLR3         0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n\n                 91         92         93         94         95         96   \\\ngene_name                                                                     \nSLC20A1     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \nC11orf58    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \nZSCAN9     59.330220  59.330220  59.330220  59.330220  59.330220  59.330220   \nCD19       45.327386  45.327386  45.327386  45.327386  45.327386  45.327386   \nTMEM123     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n...              ...        ...        ...        ...        ...        ...   \nACOX1       0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \nMLXIP       0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \nASGR2       0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \nOR5A1       0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \nTLR3        0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n\n                 97         98         99         100  \ngene_name                                              \nSLC20A1     0.000000   0.000000   0.000000   0.000000  \nC11orf58    0.000000   0.000000   0.000000   0.000000  \nZSCAN9     59.330220  59.330220  59.330220  59.330220  \nCD19       45.327386  45.327386  45.327386  45.327386  \nTMEM123     0.000000   0.000000   0.000000   0.000000  \n...              ...        ...        ...        ...  \nACOX1       0.000000   0.000000   0.000000   0.000000  \nMLXIP       0.000000   0.000000   0.000000   0.000000  \nASGR2       0.000000   0.000000   0.000000   0.000000  \nOR5A1       0.000000   0.000000   0.000000   0.000000  \nTLR3        0.000000   0.000000   0.000000   0.000000  \n\n[14310 rows x 201 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-100</th>\n      <th>-99</th>\n      <th>-98</th>\n      <th>-97</th>\n      <th>-96</th>\n      <th>-95</th>\n      <th>-94</th>\n      <th>-93</th>\n      <th>-92</th>\n      <th>-91</th>\n      <th>...</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n      <th>100</th>\n    </tr>\n    <tr>\n      <th>gene_name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>SLC20A1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>C11orf58</th>\n      <td>107.928289</td>\n      <td>107.928289</td>\n      <td>107.928289</td>\n      <td>107.928289</td>\n      <td>107.928289</td>\n      <td>107.928289</td>\n      <td>107.928289</td>\n      <td>107.928289</td>\n      <td>107.928289</td>\n      <td>107.928289</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>ZSCAN9</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>59.330220</td>\n      <td>59.330220</td>\n      <td>59.330220</td>\n      <td>59.330220</td>\n      <td>59.330220</td>\n      <td>59.330220</td>\n      <td>59.330220</td>\n      <td>59.330220</td>\n      <td>59.330220</td>\n      <td>59.330220</td>\n    </tr>\n    <tr>\n      <th>CD19</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>45.327386</td>\n      <td>45.327386</td>\n      <td>45.327386</td>\n      <td>45.327386</td>\n      <td>45.327386</td>\n      <td>45.327386</td>\n      <td>45.327386</td>\n      <td>45.327386</td>\n      <td>45.327386</td>\n      <td>45.327386</td>\n    </tr>\n    <tr>\n      <th>TMEM123</th>\n      <td>38.737818</td>\n      <td>38.737818</td>\n      <td>38.737818</td>\n      <td>38.737818</td>\n      <td>38.737818</td>\n      <td>38.737818</td>\n      <td>38.737818</td>\n      <td>38.737818</td>\n      <td>38.737818</td>\n      <td>38.737818</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>ACOX1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>MLXIP</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>ASGR2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>OR5A1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>TLR3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>14310 rows Ã— 201 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(bed_file_paths_X1[0], train_info_X1_path,100,1,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "### FUNCTION FOR CREATING TRAINING DATASET\n",
    "def create_set(bed_paths, df_info, max_distance, resolution, stride, use_score=True):\n",
    "    \"\"\"\n",
    "    Create training dataset\n",
    "    :param bed_paths:\n",
    "    :param df_info:\n",
    "    :param max_distance:\n",
    "    :param resolution:\n",
    "    :param stride:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    df_train = pd.concat([extract_features(path,df_info, max_distance, resolution, stride, use_score) for path in bed_paths], axis=1)\n",
    "    df_train.columns = [i for i in range(df_train.columns.size)]\n",
    "\n",
    "    return df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me1-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K9me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27ac-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K36me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me1-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K9me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27ac-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K36me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me1-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K4me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K9me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27ac-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K27me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n",
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/H3K36me3-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X2_val_info.tsv\n",
      "Done!\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "### PREPARE DATA FOR TRAINING\n",
    "# set preprocessing parameter\n",
    "# s\n",
    "\n",
    "max_distance = 500\n",
    "resolution = 20\n",
    "stride = 10\n",
    "\n",
    "# load labels\n",
    "train_y_X1 = pd.read_csv(train_y_X1_path, delimiter=\"\\t\")\n",
    "train_x_X1 = create_set(bed_file_paths_X1, train_info_X1_path, max_distance, resolution, stride, use_score=False)\n",
    "\n",
    "val_y_X1 = pd.read_csv(val_y_X1_path, delimiter=\"\\t\")\n",
    "val_x_X1 = create_set(bed_file_paths_X1, val_info_X1_path, max_distance, resolution, stride, use_score=False)\n",
    "\n",
    "val_y_X2 = pd.read_csv(val_y_X2_path, delimiter=\"\\t\")\n",
    "val_x_X2 = create_set(bed_file_paths_X2, val_info_X2_path, max_distance, resolution, stride, use_score=False)\n",
    "\n",
    "# store datasets\n",
    "train_x_X1.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/train_x_X1_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)\n",
    "val_x_X1.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X1_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)\n",
    "val_x_X2.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X2_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "### LOAD DATASETS\n",
    "max_distance = 10000\n",
    "resolution = 500\n",
    "stride = 250\n",
    "\n",
    "train_y_X1 = pd.read_csv(train_y_X1_path, delimiter=\"\\t\")\n",
    "train_x_X1 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/train_x_X1_\"+\n",
    "                         str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n",
    "\n",
    "val_y_X1 = pd.read_csv(val_y_X1_path, delimiter=\"\\t\")\n",
    "val_x_X1 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X1_\"+\n",
    "                       str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n",
    "\n",
    "val_y_X2 = pd.read_csv(val_y_X2_path, delimiter=\"\\t\")\n",
    "val_x_X2 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X2_\"+\n",
    "                       str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work Package 1.2 - Model Building"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-map:0.88516\ttrain-map:0.87375\n",
      "[1]\tvalidation-map:0.89378\ttrain-map:0.88973\n",
      "[2]\tvalidation-map:0.88978\ttrain-map:0.89156\n",
      "[3]\tvalidation-map:0.89031\ttrain-map:0.89460\n",
      "[4]\tvalidation-map:0.89458\ttrain-map:0.89723\n",
      "[5]\tvalidation-map:0.89492\ttrain-map:0.90082\n",
      "[6]\tvalidation-map:0.89464\ttrain-map:0.90186\n",
      "[7]\tvalidation-map:0.89538\ttrain-map:0.90189\n",
      "[8]\tvalidation-map:0.89707\ttrain-map:0.90368\n",
      "[9]\tvalidation-map:0.89867\ttrain-map:0.90543\n",
      "[10]\tvalidation-map:0.89883\ttrain-map:0.90675\n",
      "RMSE of the base model: 276.650\n",
      "SPMC of the base model: 0.698\n"
     ]
    }
   ],
   "source": [
    "### XGBOOST MODEL\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create regression matrices\n",
    "dtrain_reg = xgb.DMatrix(train_x_X1, train_y_X1[\"gex\"], enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(val_x_X1, val_y_X1[\"gex\"], enable_categorical=True)\n",
    "\n",
    "# Define training parameters\n",
    "params = {\"objective\": \"rank:pairwise\"}\n",
    "evals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n",
    "n = 11\n",
    "\n",
    "# Train xgboost model\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain_reg,\n",
    "    num_boost_round=n,\n",
    "    evals=evals,\n",
    "    verbose_eval=1,\n",
    "    # Activate early stopping\n",
    "    early_stopping_rounds=1\n",
    ")\n",
    "\n",
    "# Make predictions and score\n",
    "dtest_reg = xgb.DMatrix(val_x_X2, val_y_X2[\"gex\"], enable_categorical=True)\n",
    "preds = model.predict(dtest_reg)\n",
    "rmse = mean_squared_error(val_y_X2[\"gex\"], preds, squared=False)\n",
    "spmc = stats.spearmanr(preds, val_y_X2[\"gex\"]).statistic\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "print(f\"SPMC of the base model: {spmc:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "\n",
    "def score_func(y, y_pred):\n",
    "    return spearmanr(y,y_pred).statistic\n",
    "scorer=make_scorer(score_func)\n",
    "\n",
    "dtrain_reg = xgb.DMatrix(train_x_X1, train_y_X1[\"gex\"], enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(val_x_X1, val_y_X1[\"gex\"], enable_categorical=True)\n",
    "\n",
    "model=xgb.XGBRegressor(booster='gbtree')\n",
    "param_grid={\n",
    "    'n_estimators': Integer(60,200),\n",
    "    'learning_rate': Real(1e-5,1e-1)\n",
    "\n",
    "                          ,prior='log-uniform'),\n",
    "    'max_depth': Integer(1,10),\n",
    "}\n",
    "\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=scorer,\n",
    "    n_iter=100,\n",
    "    random_state=7,\n",
    "    cv=5,\n",
    "    verbose=3)\n",
    "\n",
    "opt.fit(train_x_X1, train_y_X1[\"gex\"])\n",
    "\n",
    "print(f'Best CV on same Cell Line Score: {opt.best_score_}')\n",
    "print(f'Best params: {opt.best_params_}')\n",
    "y_hat_2= opt.predict(val_x_X2)\n",
    "score=spearmanr(y_hat_2, val_y_X2)\n",
    "print(f'Score on different Cell Line: {score}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_hat_2= opt.predict(val_x_X2)\n",
    "score=spearmanr(y_hat_2, val_y_X2['gex'])\n",
    "print(f'Score on different Cell Line: {score.statistic}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_hat_2.shape\n",
    "val_y_X2.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/30 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f1e376f4da343ee897e7d62d1e9a5f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m version \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m tpot \u001B[38;5;241m=\u001B[39m TPOTRegressor(generations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, population_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, verbosity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mtpot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x_X1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_y_X1\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgex\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m tpot\u001B[38;5;241m.\u001B[39mexport(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mteapots/teapot_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39mversion\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.py\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/tpot/base.py:863\u001B[0m, in \u001B[0;36mTPOTBase.fit\u001B[0;34m(self, features, target, sample_weight, groups)\u001B[0m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mSystemExit\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    861\u001B[0m         \u001B[38;5;66;03m# raise the exception if it's our last attempt\u001B[39;00m\n\u001B[1;32m    862\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m attempt \u001B[38;5;241m==\u001B[39m (attempts \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m--> 863\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    864\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/tpot/base.py:854\u001B[0m, in \u001B[0;36mTPOTBase.fit\u001B[0;34m(self, features, target, sample_weight, groups)\u001B[0m\n\u001B[1;32m    851\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pbar, \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[1;32m    852\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pbar\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m--> 854\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_top_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_summary_of_best_pipeline(features, target)\n\u001B[1;32m    856\u001B[0m \u001B[38;5;66;03m# Delete the temporary cache before exiting\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/tpot/base.py:961\u001B[0m, in \u001B[0;36mTPOTBase._update_top_pipeline\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    957\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_optimized_pareto_front_n_gens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    958\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    959\u001B[0m     \u001B[38;5;66;03m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001B[39;00m\n\u001B[1;32m    960\u001B[0m     \u001B[38;5;66;03m# need raise RuntimeError because no pipeline has been optimized\u001B[39;00m\n\u001B[0;32m--> 961\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    962\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA pipeline has not yet been optimized. Please call fit() first.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    963\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "### TEAPOT ANALYSIS\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "#Run teapot analysis\n",
    "version = \"1.2\"\n",
    "tpot = TPOTRegressor(generations=5, population_size=5, verbosity=2, random_state=42)\n",
    "tpot.fit(train_x_X1.to_numpy(), train_y_X1[\"gex\"].to_numpy())\n",
    "tpot.export('teapots/teapot_'+version+'.py')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "preds = tpot.predict(val_x_X2.to_numpy())\n",
    "rmse = mean_squared_error(val_y_X2[\"gex\"], preds, squared=False)\n",
    "spmc = stats.spearmanr(preds, val_y_X2[\"gex\"]).statistic\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "print(f\"SPMC of the base model: {spmc:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Using the model trained in WP 1.2, make predictions on the test data (chr 1 of cell line X3).\n",
    "# Store predictions in a variable called \"pred\" which is a numpy array.\n",
    "\n",
    "pred = None\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Check if \"pred\" meets the specified constrains\n",
    "assert isinstance(pred, np.ndarray), 'Prediction array must be a numpy array'\n",
    "assert np.issubdtype(pred.dtype, np.number), 'Prediction array must be numeric'\n",
    "assert pred.shape[0] == len(test_genes), 'Each gene should have a unique predicted expression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = 'path/to/save/output/file'  # TODO\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"LastName_FirstName_Project1.zip\" # TODO\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "test_genes['gex_predicted'] = pred.tolist()\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
