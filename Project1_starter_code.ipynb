{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that are required to run your project\n",
    "# You are allowed to add more libraries as you need\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PATHS\n",
    "\n",
    "# NOTE: \n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you. \n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "# Path for datasets\n",
    "path_cwd = os.getcwd()\n",
    "path_data = path_cwd+\"/ML4G_Project_1_Data\"\n",
    "\n",
    "# Metadata for genes of cell lines X1 and X2\n",
    "train_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_info.tsv\"\n",
    "train_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_info.tsv\"\n",
    "val_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_info.tsv\"\n",
    "val_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_info.tsv\"\n",
    "\n",
    "# Gene expression values for cell lines X1 and X2\n",
    "train_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_y.tsv\"\n",
    "train_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_y.tsv\"\n",
    "val_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_y.tsv\"\n",
    "val_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_y.tsv\"\n",
    "\n",
    "# DNase and histone modification data for cell lines X1, X2 and X3\n",
    "bed_files_X1 = [\"/DNase-bed/X1.bed\",\n",
    "                \"/H3K4me1-bed/X1.bed\",\n",
    "                \"/H3K4me3-bed/X1.bed\",\n",
    "                \"/H3K9me3-bed/X1.bed\",\n",
    "                \"/H3K27ac-bed/X1.bed\",\n",
    "                \"/H3K27me3-bed/X1.bed\",\n",
    "                \"/H3K36me3-bed/X1.bed\"]\n",
    "bed_file_paths_X1 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X2 = [\"/DNase-bed/X2.bed\",\n",
    "                \"/H3K4me1-bed/X2.bed\",\n",
    "                \"/H3K4me3-bed/X2.bed\",\n",
    "                \"/H3K9me3-bed/X2.bed\",\n",
    "                \"/H3K27ac-bed/X2.bed\",\n",
    "                \"/H3K27me3-bed/X2.bed\",\n",
    "                \"/H3K36me3-bed/X2.bed\"]\n",
    "bed_file_paths_X2 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X3 = [\"/DNase-bed/X3.bed\",\n",
    "                \"/H3K4me1-bed/X3.bed\",\n",
    "                \"/H3K4me3-bed/X3.bed\",\n",
    "                \"/H3K9me3-bed/X3.bed\",\n",
    "                \"/H3K27ac-bed/X3.bed\",\n",
    "                \"/H3K27me3-bed/X3.bed\",\n",
    "                \"/H3K36me3-bed/X3.bed\"]\n",
    "bed_file_paths_X3 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "# Small dataset for debugging\n",
    "debug_info_path = path_data+\"/info.tsv\"\n",
    "debug_bed_file = path_data+\"/bed_file.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "### FUNCTION FOR EXTRACTION OF FEATURES\n",
    "def extract_features(bed_path, info_path, max_distance, resolution, stride):\n",
    "    \"\"\"\n",
    "    Function extracting binary features from bed datasets\n",
    "    :param bed_path: path to bed file of interest\n",
    "    :param info_path: path to info file of interest\n",
    "    :param max_distance: maximal distance from TSS that should be considered\n",
    "    :param resolution: window size of aggregation for dimensionality reduction\n",
    "    :param stride: stride for dimensionality reduction\n",
    "    :return: pandas df of type int8 containing binary features\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df_info = pd.read_csv(info_path, sep='\\t', usecols=[0,1,4])\n",
    "    df_peak_data = pd.read_csv(bed_path, sep='\\t', usecols=[0,1,2], names = [\"chromosome\", \"peak_start\", \"peak_end\"])\n",
    "\n",
    "    # Get genes and initialize features df with False as entries\n",
    "    df_features = pd.DataFrame(data=0,columns=[i-max_distance-1 for i in range(1, 2*(max_distance+1))], index=df_info[\"gene_name\"], dtype=\"int8\")\n",
    "\n",
    "    # Fill df according to info data\n",
    "    for i in df_info.index:\n",
    "        gene = df_info[\"gene_name\"][i]\n",
    "        tss = df_info[\"TSS_start\"][i]\n",
    "        chromosome = df_info[\"chr\"][i]\n",
    "        tss_l = tss - max_distance\n",
    "        tss_r = tss + max_distance\n",
    "\n",
    "        # Print progress\n",
    "        if i == 0:\n",
    "            print(\"Start preprocessing of:\", \"\\n\"+\n",
    "                  \"Dataset:\", bed_path, \"\\n\"+\n",
    "                  \"Infoset:\", info_path)\n",
    "        if i == df_info.index[-1]:\n",
    "            print(\"Done!\" + \"\\n\" + \"-----------------------------------\")\n",
    "\n",
    "        # Find relevant peaks\n",
    "        peaks = df_peak_data.loc[(df_peak_data[\"peak_start\"] <= tss_r) &\n",
    "                                 (df_peak_data[\"peak_end\"] >= tss_l)]\n",
    "\n",
    "        # Fill features dataset\n",
    "        for j in range(peaks.shape[0]):\n",
    "            # Make sure that peak is on the same chromosome\n",
    "            if peaks[\"chromosome\"].iloc[j] != chromosome: continue\n",
    "\n",
    "            # Get peak boundaries\n",
    "            peak_l = peaks[\"peak_start\"].iloc[j]\n",
    "            peak_r = peaks[\"peak_end\"].iloc[j]\n",
    "\n",
    "            # Consider possible cases\n",
    "            if (peak_l >= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : peak_r-tss] = 1\n",
    "\n",
    "            elif (peak_l <= tss_r) and (peak_r >= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : tss_r-tss] = 1\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : peak_r-tss] = 1\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r >= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : tss_r-tss] = 1\n",
    "\n",
    "    # Introduce resolution (rather inefficient...)\n",
    "    df_features=df_features.rolling(window=resolution,\n",
    "                                      axis=1,\n",
    "                                      step=stride,\n",
    "                                      min_periods=1,\n",
    "                                      center=True).mean()\n",
    "\n",
    "    # df_features[df_features >= 0.5] = 1\n",
    "    # df_features[df_features < 0.5] = 0\n",
    "\n",
    "    return df_features#.astype(\"int8\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "### FUNCTION FOR CREATING TRAINING DATASET\n",
    "def create_set(bed_paths, df_info, max_distance, resolution, stride):\n",
    "    \"\"\"\n",
    "    Create training dataset\n",
    "    :param bed_paths:\n",
    "    :param df_info:\n",
    "    :param max_distance:\n",
    "    :param resolution:\n",
    "    :param stride:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    df_train = pd.concat([extract_features(path,df_info, max_distance, resolution, stride) for path in bed_paths], axis=1)\n",
    "    df_train.columns = [i for i in range(df_train.columns.size)]\n",
    "\n",
    "    return df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing of: \n",
      "Dataset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/DNase-bed/X1.bed \n",
      "Infoset: /home/mike/Masters_DS/ml4g_2023/ML4G_Project_1_Data/CAGE-train/CAGE-train/X1_train_info.tsv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# load labels\u001B[39;00m\n\u001B[1;32m     10\u001B[0m train_y_X1 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(train_y_X1_path, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m train_x_X1 \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_set\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbed_file_paths_X1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_info_X1_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_distance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresolution\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m val_y_X1 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(val_y_X1_path, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m val_x_X1 \u001B[38;5;241m=\u001B[39m create_set(bed_file_paths_X1, val_info_X1_path, max_distance, resolution, stride)\n",
      "Cell \u001B[0;32mIn[5], line 13\u001B[0m, in \u001B[0;36mcreate_set\u001B[0;34m(bed_paths, df_info, max_distance, resolution, stride)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_set\u001B[39m(bed_paths, df_info, max_distance, resolution, stride):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    Create training dataset\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m    :param bed_paths:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m    :return:\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m     df_train \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([extract_features(path,df_info, max_distance, resolution, stride) \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m bed_paths], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     14\u001B[0m     df_train\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(df_train\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39msize)]\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df_train\n",
      "Cell \u001B[0;32mIn[5], line 13\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_set\u001B[39m(bed_paths, df_info, max_distance, resolution, stride):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    Create training dataset\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m    :param bed_paths:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m    :return:\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m     df_train \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdf_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_distance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresolution\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m bed_paths], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     14\u001B[0m     df_train\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(df_train\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39msize)]\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df_train\n",
      "Cell \u001B[0;32mIn[4], line 37\u001B[0m, in \u001B[0;36mextract_features\u001B[0;34m(bed_path, info_path, max_distance, resolution, stride)\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDone!\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-----------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Find relevant peaks\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m peaks \u001B[38;5;241m=\u001B[39m \u001B[43mdf_peak_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_peak_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpeak_start\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtss_r\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\n\u001B[1;32m     38\u001B[0m \u001B[43m                         \u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_peak_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpeak_end\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtss_l\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Fill features dataset\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(peaks\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;66;03m# Make sure that peak is on the same chromosome\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/indexing.py:1073\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1070\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1072\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[0;32m-> 1073\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/indexing.py:1292\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1290\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_slice_axis(key, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[1;32m   1291\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m com\u001B[38;5;241m.\u001B[39mis_bool_indexer(key):\n\u001B[0;32m-> 1292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getbool_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_list_like_indexer(key):\n\u001B[1;32m   1294\u001B[0m \n\u001B[1;32m   1295\u001B[0m     \u001B[38;5;66;03m# an iterable multi-selection\u001B[39;00m\n\u001B[1;32m   1296\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(labels, MultiIndex)):\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/indexing.py:1093\u001B[0m, in \u001B[0;36m_LocationIndexer._getbool_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1091\u001B[0m key \u001B[38;5;241m=\u001B[39m check_bool_indexer(labels, key)\n\u001B[1;32m   1092\u001B[0m inds \u001B[38;5;241m=\u001B[39m key\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m-> 1093\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take_with_is_copy\u001B[49m\u001B[43m(\u001B[49m\u001B[43minds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/generic.py:3902\u001B[0m, in \u001B[0;36mNDFrame._take_with_is_copy\u001B[0;34m(self, indices, axis)\u001B[0m\n\u001B[1;32m   3894\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_take_with_is_copy\u001B[39m(\u001B[38;5;28mself\u001B[39m: NDFrameT, indices, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NDFrameT:\n\u001B[1;32m   3895\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3896\u001B[0m \u001B[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001B[39;00m\n\u001B[1;32m   3897\u001B[0m \u001B[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3900\u001B[0m \u001B[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001B[39;00m\n\u001B[1;32m   3901\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3902\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3903\u001B[0m     \u001B[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001B[39;00m\n\u001B[1;32m   3904\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m result\u001B[38;5;241m.\u001B[39m_get_axis(axis)\u001B[38;5;241m.\u001B[39mequals(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_axis(axis)):\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/generic.py:3886\u001B[0m, in \u001B[0;36mNDFrame._take\u001B[0;34m(self, indices, axis, convert_indices)\u001B[0m\n\u001B[1;32m   3879\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3880\u001B[0m \u001B[38;5;124;03mInternal version of the `take` allowing specification of additional args.\u001B[39;00m\n\u001B[1;32m   3881\u001B[0m \n\u001B[1;32m   3882\u001B[0m \u001B[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001B[39;00m\n\u001B[1;32m   3883\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3884\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consolidate_inplace()\n\u001B[0;32m-> 3886\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3887\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3888\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_block_manager_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3889\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   3890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3891\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3892\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor(new_data)\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtake\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/internals/managers.py:977\u001B[0m, in \u001B[0;36mBaseBlockManager.take\u001B[0;34m(self, indexer, axis, verify, convert_indices)\u001B[0m\n\u001B[1;32m    974\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_indices:\n\u001B[1;32m    975\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m maybe_convert_indices(indexer, n, verify\u001B[38;5;241m=\u001B[39mverify)\n\u001B[0;32m--> 977\u001B[0m new_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxes\u001B[49m\u001B[43m[\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreindex_indexer(\n\u001B[1;32m    979\u001B[0m     new_axis\u001B[38;5;241m=\u001B[39mnew_labels,\n\u001B[1;32m    980\u001B[0m     indexer\u001B[38;5;241m=\u001B[39mindexer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    983\u001B[0m     copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    984\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/pandas/core/indexes/base.py:1182\u001B[0m, in \u001B[0;36mIndex.take\u001B[0;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001B[0m\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# Note: we discard fill_value and use self._na_value, only relevant\u001B[39;00m\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;66;03m#  in the case where allow_fill is True and fill_value is not None\u001B[39;00m\n\u001B[1;32m   1181\u001B[0m values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m-> 1182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(values, \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndarray\u001B[49m):\n\u001B[1;32m   1183\u001B[0m     taken \u001B[38;5;241m=\u001B[39m algos\u001B[38;5;241m.\u001B[39mtake(\n\u001B[1;32m   1184\u001B[0m         values, indices, allow_fill\u001B[38;5;241m=\u001B[39mallow_fill, fill_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_na_value\n\u001B[1;32m   1185\u001B[0m     )\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1187\u001B[0m     \u001B[38;5;66;03m# algos.take passes 'axis' keyword which not all EAs accept\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "### PREPARE DATA FOR TRAINING\n",
    "# set preprocessing parameter\n",
    "# s\n",
    "\n",
    "max_distance = 10000\n",
    "resolution = 500\n",
    "stride = 250\n",
    "\n",
    "# load labels\n",
    "train_y_X1 = pd.read_csv(train_y_X1_path, delimiter=\"\\t\")\n",
    "train_x_X1 = create_set(bed_file_paths_X1, train_info_X1_path, max_distance, resolution, stride)\n",
    "\n",
    "val_y_X1 = pd.read_csv(val_y_X1_path, delimiter=\"\\t\")\n",
    "val_x_X1 = create_set(bed_file_paths_X1, val_info_X1_path, max_distance, resolution, stride)\n",
    "\n",
    "val_y_X2 = pd.read_csv(val_y_X2_path, delimiter=\"\\t\")\n",
    "val_x_X2 = create_set(bed_file_paths_X2, val_info_X2_path, max_distance, resolution, stride)\n",
    "\n",
    "# store datasets\n",
    "train_x_X1.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/train_x_X1_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)\n",
    "val_x_X1.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X1_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)\n",
    "val_x_X2.to_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X2_\"+str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "### LOAD DATASETS\n",
    "max_distance = 10000\n",
    "resolution = 500\n",
    "stride = 250\n",
    "\n",
    "train_y_X1 = pd.read_csv(train_y_X1_path, delimiter=\"\\t\")\n",
    "train_x_X1 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/train_x_X1_\"+\n",
    "                         str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n",
    "\n",
    "val_y_X1 = pd.read_csv(val_y_X1_path, delimiter=\"\\t\")\n",
    "val_x_X1 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X1_\"+\n",
    "                       str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n",
    "\n",
    "val_y_X2 = pd.read_csv(val_y_X2_path, delimiter=\"\\t\")\n",
    "val_x_X2 = pd.read_csv(\"ML4G_Project_1_Data/Preprocessed-train/val_x_X2_\"+\n",
    "                       str(max_distance)+\"_\"+str(resolution)+\"_\"+str(stride)+\".csv\", index_col=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work Package 1.2 - Model Building"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-map:0.86467\ttrain-map:0.85376\n",
      "[1]\tvalidation-map:0.88105\ttrain-map:0.87631\n",
      "[2]\tvalidation-map:0.88482\ttrain-map:0.88227\n",
      "[3]\tvalidation-map:0.89219\ttrain-map:0.88665\n",
      "[4]\tvalidation-map:0.89748\ttrain-map:0.89313\n",
      "[5]\tvalidation-map:0.89530\ttrain-map:0.89369\n",
      "[6]\tvalidation-map:0.89662\ttrain-map:0.89833\n",
      "[7]\tvalidation-map:0.89399\ttrain-map:0.89918\n",
      "[8]\tvalidation-map:0.90167\ttrain-map:0.90216\n",
      "[9]\tvalidation-map:0.90242\ttrain-map:0.90350\n",
      "[10]\tvalidation-map:0.90306\ttrain-map:0.90630\n",
      "RMSE of the base model: 276.657\n",
      "SPMC of the base model: 0.682\n"
     ]
    }
   ],
   "source": [
    "### XGBOOST MODEL\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create regression matrices\n",
    "dtrain_reg = xgb.DMatrix(train_x_X1, train_y_X1[\"gex\"], enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(val_x_X1, val_y_X1[\"gex\"], enable_categorical=True)\n",
    "\n",
    "# Define training parameters\n",
    "params = {\"objective\": \"rank:pairwise\"}\n",
    "evals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n",
    "n = 11\n",
    "\n",
    "# Train xgboost model\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain_reg,\n",
    "    num_boost_round=n,\n",
    "    evals=evals,\n",
    "    verbose_eval=1,\n",
    "    # Activate early stopping\n",
    "    early_stopping_rounds=1\n",
    ")\n",
    "\n",
    "# Make predictions and score\n",
    "dtest_reg = xgb.DMatrix(val_x_X2, val_y_X2[\"gex\"], enable_categorical=True)\n",
    "preds = model.predict(dtest_reg)\n",
    "rmse = mean_squared_error(val_y_X2[\"gex\"], preds, squared=False)\n",
    "spmc = stats.spearmanr(preds, val_y_X2[\"gex\"]).statistic\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "print(f\"SPMC of the base model: {spmc:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "\n",
    "def score_func(y, y_pred):\n",
    "    return spearmanr(y,y_pred).statistic\n",
    "scorer=make_scorer(score_func)\n",
    "\n",
    "dtrain_reg = xgb.DMatrix(train_x_X1, train_y_X1[\"gex\"], enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(val_x_X1, val_y_X1[\"gex\"], enable_categorical=True)\n",
    "\n",
    "model=xgb.XGBRegressor(booster='gbtree')\n",
    "param_grid={\n",
    "    'n_estimators': Integer(60,200),\n",
    "    'learning_rate': Real(1e-5,1e-1)\n",
    "\n",
    "                          ,prior='log-uniform'),\n",
    "    'max_depth': Integer(1,10),\n",
    "}\n",
    "\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=scorer,\n",
    "    n_iter=100,\n",
    "    random_state=7,\n",
    "    cv=5,\n",
    "    verbose=3)\n",
    "\n",
    "opt.fit(train_x_X1, train_y_X1[\"gex\"])\n",
    "\n",
    "print(f'Best CV on same Cell Line Score: {opt.best_score_}')\n",
    "print(f'Best params: {opt.best_params_}')\n",
    "y_hat_2= opt.predict(val_x_X2)\n",
    "score=spearmanr(y_hat_2, val_y_X2)\n",
    "print(f'Score on different Cell Line: {score}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_hat_2= opt.predict(val_x_X2)\n",
    "score=spearmanr(y_hat_2, val_y_X2['gex'])\n",
    "print(f'Score on different Cell Line: {score.statistic}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_hat_2.shape\n",
    "val_y_X2.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/30 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f1e376f4da343ee897e7d62d1e9a5f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m version \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m tpot \u001B[38;5;241m=\u001B[39m TPOTRegressor(generations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, population_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, verbosity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mtpot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x_X1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_y_X1\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgex\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m tpot\u001B[38;5;241m.\u001B[39mexport(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mteapots/teapot_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39mversion\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.py\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/tpot/base.py:863\u001B[0m, in \u001B[0;36mTPOTBase.fit\u001B[0;34m(self, features, target, sample_weight, groups)\u001B[0m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mSystemExit\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    861\u001B[0m         \u001B[38;5;66;03m# raise the exception if it's our last attempt\u001B[39;00m\n\u001B[1;32m    862\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m attempt \u001B[38;5;241m==\u001B[39m (attempts \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m--> 863\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    864\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/tpot/base.py:854\u001B[0m, in \u001B[0;36mTPOTBase.fit\u001B[0;34m(self, features, target, sample_weight, groups)\u001B[0m\n\u001B[1;32m    851\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pbar, \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[1;32m    852\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pbar\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m--> 854\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_top_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_summary_of_best_pipeline(features, target)\n\u001B[1;32m    856\u001B[0m \u001B[38;5;66;03m# Delete the temporary cache before exiting\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/eth_ml4g/lib/python3.9/site-packages/tpot/base.py:961\u001B[0m, in \u001B[0;36mTPOTBase._update_top_pipeline\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    957\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_optimized_pareto_front_n_gens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    958\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    959\u001B[0m     \u001B[38;5;66;03m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001B[39;00m\n\u001B[1;32m    960\u001B[0m     \u001B[38;5;66;03m# need raise RuntimeError because no pipeline has been optimized\u001B[39;00m\n\u001B[0;32m--> 961\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    962\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA pipeline has not yet been optimized. Please call fit() first.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    963\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "### TEAPOT ANALYSIS\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "#Run teapot analysis\n",
    "version = \"1.2\"\n",
    "tpot = TPOTRegressor(generations=5, population_size=5, verbosity=2, random_state=42)\n",
    "tpot.fit(train_x_X1.to_numpy(), train_y_X1[\"gex\"].to_numpy())\n",
    "tpot.export('teapots/teapot_'+version+'.py')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "preds = tpot.predict(val_x_X2.to_numpy())\n",
    "rmse = mean_squared_error(val_y_X2[\"gex\"], preds, squared=False)\n",
    "spmc = stats.spearmanr(preds, val_y_X2[\"gex\"]).statistic\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "print(f\"SPMC of the base model: {spmc:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Using the model trained in WP 1.2, make predictions on the test data (chr 1 of cell line X3).\n",
    "# Store predictions in a variable called \"pred\" which is a numpy array.\n",
    "\n",
    "pred = None\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Check if \"pred\" meets the specified constrains\n",
    "assert isinstance(pred, np.ndarray), 'Prediction array must be a numpy array'\n",
    "assert np.issubdtype(pred.dtype, np.number), 'Prediction array must be numeric'\n",
    "assert pred.shape[0] == len(test_genes), 'Each gene should have a unique predicted expression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = 'path/to/save/output/file'  # TODO\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"LastName_FirstName_Project1.zip\" # TODO\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "test_genes['gex_predicted'] = pred.tolist()\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
