{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORTANT ! Note that an additional folder ('Results') needs to be created\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you.\n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "# Path for datasets\n",
    "path_cwd = os.getcwd()\n",
    "path_data = path_cwd+\"/ML4G_Project_1_Data\"\n",
    "\n",
    "# Metadata for genes of cell lines X1 and X2\n",
    "train_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_info.tsv\"\n",
    "train_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_info.tsv\"\n",
    "val_info_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_info.tsv\"\n",
    "val_info_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_info.tsv\"\n",
    "\n",
    "# Gene expression values for cell lines X1 and X2\n",
    "train_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_train_y.tsv\"\n",
    "train_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_train_y.tsv\"\n",
    "val_y_X1_path = path_data+\"/CAGE-train/CAGE-train/X1_val_y.tsv\"\n",
    "val_y_X2_path = path_data+\"/CAGE-train/CAGE-train/X2_val_y.tsv\"\n",
    "\n",
    "# DNase and histone modification data for cell lines X1, X2 and X3\n",
    "bed_files_X1 = [\"/DNase-bed/X1.bed\",\n",
    "                \"/H3K4me1-bed/X1.bed\",\n",
    "                \"/H3K4me3-bed/X1.bed\",\n",
    "                \"/H3K9me3-bed/X1.bed\",\n",
    "                \"/H3K27ac-bed/X1.bed\",\n",
    "                \"/H3K27me3-bed/X1.bed\",\n",
    "                \"/H3K36me3-bed/X1.bed\"]\n",
    "bed_file_paths_X1 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X2 = [\"/DNase-bed/X2.bed\",\n",
    "                \"/H3K4me1-bed/X2.bed\",\n",
    "                \"/H3K4me3-bed/X2.bed\",\n",
    "                \"/H3K9me3-bed/X2.bed\",\n",
    "                \"/H3K27ac-bed/X2.bed\",\n",
    "                \"/H3K27me3-bed/X2.bed\",\n",
    "                \"/H3K36me3-bed/X2.bed\"]\n",
    "bed_file_paths_X2 = [path_data+file for file in bed_files_X1]\n",
    "\n",
    "bed_files_X3 = [\"/DNase-bed/X3.bed\",\n",
    "                \"/H3K4me1-bed/X3.bed\",\n",
    "                \"/H3K4me3-bed/X3.bed\",\n",
    "                \"/H3K9me3-bed/X3.bed\",\n",
    "                \"/H3K27ac-bed/X3.bed\",\n",
    "                \"/H3K27me3-bed/X3.bed\",\n",
    "                \"/H3K36me3-bed/X3.bed\"]\n",
    "bed_file_paths_X3 = [path_data+file for file in bed_files_X1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "### FUNCTION FOR EXTRACTION OF FEATURES\n",
    "def extract_features(bed_path, info_path, max_distance, resolution, stride, verbose=0, use_score=True):\n",
    "    \"\"\"\n",
    "    Function extracting binary features from bed datasets\n",
    "    :param bed_path: path to bed file of interest\n",
    "    :param info_path: path to info file of interest\n",
    "    :param max_distance: maximal distance from TSS that should be considered\n",
    "    :param resolution: window size of aggregation for dimensionality reduction\n",
    "    :param stride: stride for dimensionality reduction\n",
    "    :return: pandas df of type int8 containing binary features\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df_info = pd.read_csv(info_path, sep='\\t', usecols=[0,1,4])\n",
    "\n",
    "    # Get peak data with score column\n",
    "    if (\"DNase\" in bed_path):\n",
    "        score_col = 6\n",
    "    else: score_col = 4\n",
    "\n",
    "    df_peak_data = pd.read_csv(bed_path, sep='\\t', usecols=[0,1,2,score_col], names = [\"chromosome\", \"peak_start\", \"peak_end\", \"score\"])\n",
    "\n",
    "    # Get genes and initialize features df with False as entries\n",
    "    df_features = pd.DataFrame(data=0,columns=[i-max_distance-1 for i in range(1, 2*(max_distance+1))], index=df_info[\"gene_name\"])\n",
    "\n",
    "    # Fill df according to info data\n",
    "    for i in df_info.index:\n",
    "        gene = df_info[\"gene_name\"][i]\n",
    "        tss = df_info[\"TSS_start\"][i]\n",
    "        chromosome = df_info[\"chr\"][i]\n",
    "        tss_l = tss - max_distance\n",
    "        tss_r = tss + max_distance\n",
    "\n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            if i == 0:\n",
    "                print(\"Start preprocessing of:\", \"\\n\"+\n",
    "                      \"Dataset:\", bed_path, \"\\n\"+\n",
    "                      \"Infoset:\", info_path)\n",
    "            if i == df_info.index[-1]:\n",
    "                print(\"Done!\" + \"\\n\" + \"-----------------------------------\")\n",
    "\n",
    "        # Find relevant peaks\n",
    "        peaks = df_peak_data.loc[(df_peak_data[\"peak_start\"] <= tss_r) &\n",
    "                                 (df_peak_data[\"peak_end\"] >= tss_l)]\n",
    "\n",
    "        # Fill features dataset\n",
    "        for j in range(peaks.shape[0]):\n",
    "            # Make sure that peak is on the same chromosome\n",
    "            if peaks[\"chromosome\"].iloc[j] != chromosome: continue\n",
    "\n",
    "            # Get peak boundaries\n",
    "            peak_l = peaks[\"peak_start\"].iloc[j]\n",
    "            peak_r = peaks[\"peak_end\"].iloc[j]\n",
    "\n",
    "            if use_score:\n",
    "                # Get score\n",
    "                score = peaks[\"score\"].iloc[j]\n",
    "            else:\n",
    "                score = 1\n",
    "\n",
    "            # Consider possible cases\n",
    "            if (peak_l >= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : peak_r-tss] = score\n",
    "\n",
    "            elif (peak_l <= tss_r) and (peak_r >= tss_r):\n",
    "                df_features.loc[[gene], peak_l-tss : tss_r-tss] = score\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r <= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : peak_r-tss] = score\n",
    "\n",
    "            elif (peak_l <= tss_l) and (peak_r >= tss_r):\n",
    "                df_features.loc[[gene], tss_l-tss : tss_r-tss] = score\n",
    "\n",
    "    # Introduce resolution (rather inefficient...)\n",
    "    df_features=df_features.rolling(window=resolution,\n",
    "                                    axis=1,\n",
    "                                    step=stride,\n",
    "                                    min_periods=1,\n",
    "                                    center=True).mean()\n",
    "\n",
    "    return df_features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "### FUNCTION FOR CREATING TRAINING DATASET\n",
    "def create_set(bed_paths, df_info, max_distance, resolution, stride, verbose=1, use_score=True):\n",
    "    \"\"\"\n",
    "    Create training dataset\n",
    "    :param bed_paths:\n",
    "    :param df_info:\n",
    "    :param max_distance:\n",
    "    :param resolution:\n",
    "    :param stride:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df_train = pd.concat([extract_features(path,df_info, max_distance, resolution, stride, verbose, use_score) for path in bed_paths], axis=1)\n",
    "    df_train.columns = [i for i in range(df_train.columns.size)]\n",
    "\n",
    "    return df_train\n",
    "\n",
    "def create_set_np(bed_paths, df_info, max_distance, resolution, stride,verbose=0, use_score=True):\n",
    "    '''\n",
    "    Equivalent to function above but using numpy arrays to gain efficiency in memory and time\n",
    "    :param bed_paths:\n",
    "    :param df_info:\n",
    "    :param max_distance:\n",
    "    :param resolution:\n",
    "    :param stride:\n",
    "    :param verbose:\n",
    "    :return:\n",
    "    '''\n",
    "    for idx,path in enumerate(bed_paths):\n",
    "        n=len(bed_paths)\n",
    "        if idx==0:\n",
    "            temp=extract_features(path,df_info, max_distance, resolution, stride, verbose, use_score)\n",
    "            n_genes, n_timestamps=temp.shape\n",
    "            features=np.zeros((n_genes, n_timestamps*n))\n",
    "        else:\n",
    "            features[:,idx*n_timestamps: (idx+1)*n_timestamps]=extract_features(path,df_info, max_distance, resolution, stride,verbose).to_numpy()\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# CREATION OF SCORE FUNCTION\n",
    "def score_func(y, y_pred):\n",
    "    return spearmanr(y,y_pred).statistic\n",
    "\n",
    "scorer=make_scorer(score_func) #needed to be able to use spearmanr as score function in scikit-learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# CREATION OF COMPLETE (HPO + TESTING) TRAINING AND TESTING LOOP\n",
    "def Train_Test_loop(window_size,resolution,stride,model,inner_params,train_paths,val_paths,test_paths,mod_identifier_path='1to2',verbose=1):\n",
    "    '''\n",
    "    :param window_size:\n",
    "    :param resolution:\n",
    "    :param stride:\n",
    "    :param model:\n",
    "    :param inner_params: Dictionary describing the search space for the HPO\n",
    "    :param train_paths: look above to see how to define this and the next two parameters\n",
    "    :param val_paths:\n",
    "    :param test_paths:\n",
    "    :param verbose: regulate the printing\n",
    "    :return: a pandas dataframe summarising the result for every combination of the outer parameters and the index for the best model in that dataframe\n",
    "    '''\n",
    "    results = {}\n",
    "    results['score_test'], results['score_val'], results['time'], results['model'],results['n_features'] = [], [], [], [], []\n",
    "    for a in inner_params.keys():\n",
    "        results[a] = []\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    file_name = path_cwd+'/Models'\n",
    "    outer_params=[window_size, resolution,stride]\n",
    "    outer_params=[[i] if type(i)!= type([]) else i for i in outer_params]\n",
    "    counter, n_iter = 0, np.prod([len(x) for x in outer_params]) #to monitor training progress\n",
    "    for w in outer_params[0]:\n",
    "        for r in outer_params[1]:\n",
    "            for s in outer_params[2]:\n",
    "                counter += 1\n",
    "                print(f'Iteration {counter} out of {n_iter}\\nWINDOW: {w}, RESOLUTION: {r}, STRIDE:{s}')\n",
    "                start =time.time()\n",
    "                #creation of datasets\n",
    "                y_train = pd.read_csv(train_paths[2], delimiter=\"\\t\")['gex'].to_numpy()\n",
    "                X_train = create_set_np(train_paths[0], train_paths[1], w, r, s)\n",
    "                y_val = pd.read_csv(val_paths[2], delimiter=\"\\t\")['gex'].to_numpy()\n",
    "                X_val = create_set_np(val_paths[0], val_paths[1], w, r, s)\n",
    "                y_test = pd.read_csv(test_paths[2], delimiter=\"\\t\")['gex'].to_numpy()\n",
    "                X_test = create_set_np(test_paths[0], test_paths[1], w, r, s)\n",
    "                X_complete_train = np.concatenate([X_train,X_val])\n",
    "                y_complete_train = np.concatenate([y_train,y_val])\n",
    "                n_train = X_train.shape[0]\n",
    "                CV = [([i for i in range(n_train)],[i for i in range(n_train, y_complete_train.shape[0])])]\n",
    "                end = time.time()\n",
    "                if verbose:\n",
    "                    print(f'Number of features: {X_complete_train.shape[1]}')\n",
    "                    print(f'\\nPre-processing ended in: {round(end-start)} seconds')\n",
    "                #model definition and training\n",
    "                results['n_features'] += [X_complete_train.shape[1]]\n",
    "                model = model\n",
    "                opt = BayesSearchCV(\n",
    "                    model,\n",
    "                    inner_params,\n",
    "                    scoring = scorer,\n",
    "                    n_iter = 30,\n",
    "                    random_state = 7,\n",
    "                    cv = CV,\n",
    "                    verbose = 0)\n",
    "                start = time.time()\n",
    "                opt.fit(X_complete_train,y_complete_train)\n",
    "                end = time.time()\n",
    "                #results update\n",
    "                score = spearmanr(opt.predict(X_test),y_test).statistic\n",
    "                if verbose:\n",
    "                    print(f'Hyperparameter search ended in: {round(end-start)} seconds\\n'\n",
    "                          f'Optimal hyper parameters:{[(a,b) for a,b in opt.best_params_.items()]}\\n'\n",
    "                          f'Score in Validation: {round(opt.best_score_,4)}')\n",
    "                    print(f'Score in Test Set: {round(score,4)}\\n------------------')\n",
    "                results['score_test']+=[round(score,4)]\n",
    "                results['score_val']+=[round(opt.best_score_,4)]\n",
    "                results['time']+=[round(end-start)]\n",
    "                for a,b in opt.best_params_.items():\n",
    "                    results[a]+=[b]\n",
    "                results['model']=opt.best_estimator_\n",
    "                if score> best_score:\n",
    "                    best_model_index = (w,r,s)\n",
    "                    best_score = score\n",
    "                    pickle.dump(opt,open(path_cwd+'/Results/best_model'+ mod_identifier_path+'.pickle','wb'))\n",
    "                pickle.dump(results,open(path_cwd+'/Results/intermediate'+ mod_identifier_path+'.pickle','wb'))\n",
    "    index = pd.MultiIndex.from_product(outer_params,names=['window_size','resolution','stride'])\n",
    "    results = pd.DataFrame(results, index=index)\n",
    "    pickle.dump(results, open(path_cwd + '/Results/DF' + mod_identifier_path+ '.pickle', 'wb'))\n",
    "    return results, best_model_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# SETTING OF PARAMETERS FOR THE LOOP FUNCTION\n",
    "# Guddy Optimization:\n",
    "\n",
    "window_size=[100,300]\n",
    "resolution=[10,30]\n",
    "stride=[1,10]\n",
    "\n",
    "train_paths=[bed_file_paths_X1,train_info_X1_path, train_y_X1_path]\n",
    "val_paths=[bed_file_paths_X1,val_info_X1_path, val_y_X1_path]\n",
    "test_paths=[bed_file_paths_X2,val_info_X2_path, val_y_X2_path]\n",
    "\n",
    "\n",
    "model=xgb.XGBRegressor(booster='gbtree',objective='rank:pairwise')\n",
    "\n",
    "inner_params={\n",
    "    'n_estimators': Integer(20,150),\n",
    "    'learning_rate': Real(1e-5,1e-1,prior='log-uniform'),\n",
    "    'max_depth': Integer(1,10),\n",
    "    'reg_lambda':Integer(1,100)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 out of 8\n",
      "WINDOW: 100, RESOLUTION: 10, STRIDE:1\n",
      "Number of features: 1407\n",
      "\n",
      "Pre-processing ended in: 62 seconds\n",
      "Hyperparameter search ended in: 206 seconds\n",
      "Optimal hyper parameters:[('learning_rate', 0.1), ('max_depth', 7), ('n_estimators', 29), ('reg_lambda', 70)]\n",
      "Score in Validation: 0.7697\n",
      "Score in Test Set: 0.6805\n",
      "------------------\n",
      "Iteration 2 out of 8\n",
      "WINDOW: 100, RESOLUTION: 10, STRIDE:10\n",
      "Number of features: 147\n",
      "\n",
      "Pre-processing ended in: 63 seconds\n",
      "Hyperparameter search ended in: 40 seconds\n",
      "Optimal hyper parameters:[('learning_rate', 3.909760442914381e-05), ('max_depth', 5), ('n_estimators', 20), ('reg_lambda', 1)]\n",
      "Score in Validation: 0.7725\n",
      "Score in Test Set: 0.6853\n",
      "------------------\n",
      "Iteration 3 out of 8\n",
      "WINDOW: 100, RESOLUTION: 30, STRIDE:1\n",
      "Number of features: 1407\n",
      "\n",
      "Pre-processing ended in: 63 seconds\n",
      "Hyperparameter search ended in: 217 seconds\n",
      "Optimal hyper parameters:[('learning_rate', 0.01591799638643802), ('max_depth', 4), ('n_estimators', 88), ('reg_lambda', 9)]\n",
      "Score in Validation: 0.7745\n",
      "Score in Test Set: 0.6888\n",
      "------------------\n",
      "Iteration 4 out of 8\n",
      "WINDOW: 100, RESOLUTION: 30, STRIDE:10\n",
      "Number of features: 147\n",
      "\n",
      "Pre-processing ended in: 65 seconds\n",
      "Hyperparameter search ended in: 48 seconds\n",
      "Optimal hyper parameters:[('learning_rate', 0.1), ('max_depth', 4), ('n_estimators', 20), ('reg_lambda', 1)]\n",
      "Score in Validation: 0.7744\n",
      "Score in Test Set: 0.6909\n",
      "------------------\n",
      "Iteration 5 out of 8\n",
      "WINDOW: 300, RESOLUTION: 10, STRIDE:1\n",
      "Number of features: 4207\n",
      "\n",
      "Pre-processing ended in: 68 seconds\n",
      "Hyperparameter search ended in: 702 seconds\n",
      "Optimal hyper parameters:[('learning_rate', 0.0005408831363068807), ('max_depth', 6), ('n_estimators', 92), ('reg_lambda', 1)]\n",
      "Score in Validation: 0.7794\n",
      "Score in Test Set: 0.6938\n",
      "------------------\n",
      "Iteration 6 out of 8\n",
      "WINDOW: 300, RESOLUTION: 10, STRIDE:10\n",
      "Number of features: 427\n",
      "\n",
      "Pre-processing ended in: 67 seconds\n",
      "Hyperparameter search ended in: 99 seconds\n",
      "Optimal hyper parameters:[('learning_rate', 0.0008011047684958029), ('max_depth', 7), ('n_estimators', 20), ('reg_lambda', 1)]\n",
      "Score in Validation: 0.7811\n",
      "Score in Test Set: 0.6988\n",
      "------------------\n",
      "Iteration 7 out of 8\n",
      "WINDOW: 300, RESOLUTION: 30, STRIDE:1\n",
      "Number of features: 4207\n",
      "\n",
      "Pre-processing ended in: 67 seconds\n",
      "Hyperparameter search ended in: 698 seconds\n",
      "Optimal hyper parameters:[('learning_rate', 1.0019955549092883e-05), ('max_depth', 7), ('n_estimators', 120), ('reg_lambda', 6)]\n",
      "Score in Validation: 0.7818\n",
      "Score in Test Set: 0.6962\n",
      "------------------\n",
      "Iteration 8 out of 8\n",
      "WINDOW: 300, RESOLUTION: 30, STRIDE:10\n",
      "Number of features: 427\n",
      "\n",
      "Pre-processing ended in: 66 seconds\n",
      "Hyperparameter search ended in: 95 seconds\n",
      "Optimal hyper parameters:[('learning_rate', 1.0664169603761462e-05), ('max_depth', 7), ('n_estimators', 136), ('reg_lambda', 4)]\n",
      "Score in Validation: 0.781\n",
      "Score in Test Set: 0.6968\n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "                               score_test  score_val  time  \\\nwindow_size resolution stride                                \n100         10         1           0.6805     0.7697   206   \n                       10          0.6853     0.7725    40   \n            30         1           0.6888     0.7745   217   \n                       10          0.6909     0.7744    48   \n300         10         1           0.6938     0.7794   702   \n                       10          0.6988     0.7811    99   \n            30         1           0.6962     0.7818   698   \n                       10          0.6968     0.7810    95   \n\n                                                                           model  \\\nwindow_size resolution stride                                                      \n100         10         1       XGBRegressor(base_score=None, booster='gbtree'...   \n                       10      XGBRegressor(base_score=None, booster='gbtree'...   \n            30         1       XGBRegressor(base_score=None, booster='gbtree'...   \n                       10      XGBRegressor(base_score=None, booster='gbtree'...   \n300         10         1       XGBRegressor(base_score=None, booster='gbtree'...   \n                       10      XGBRegressor(base_score=None, booster='gbtree'...   \n            30         1       XGBRegressor(base_score=None, booster='gbtree'...   \n                       10      XGBRegressor(base_score=None, booster='gbtree'...   \n\n                               n_features  n_estimators  learning_rate  \\\nwindow_size resolution stride                                            \n100         10         1              427            29       0.100000   \n                       10             427            20       0.000039   \n            30         1              427            88       0.015918   \n                       10             427            20       0.100000   \n300         10         1              427            92       0.000541   \n                       10             427            20       0.000801   \n            30         1              427           120       0.000010   \n                       10             427           136       0.000011   \n\n                               max_depth  reg_lambda  \nwindow_size resolution stride                         \n100         10         1               7          70  \n                       10              5           1  \n            30         1               4           9  \n                       10              4           1  \n300         10         1               6           1  \n                       10              7           1  \n            30         1               7           6  \n                       10              7           4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>score_test</th>\n      <th>score_val</th>\n      <th>time</th>\n      <th>model</th>\n      <th>n_features</th>\n      <th>n_estimators</th>\n      <th>learning_rate</th>\n      <th>max_depth</th>\n      <th>reg_lambda</th>\n    </tr>\n    <tr>\n      <th>window_size</th>\n      <th>resolution</th>\n      <th>stride</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">100</th>\n      <th rowspan=\"2\" valign=\"top\">10</th>\n      <th>1</th>\n      <td>0.6805</td>\n      <td>0.7697</td>\n      <td>206</td>\n      <td>XGBRegressor(base_score=None, booster='gbtree'...</td>\n      <td>427</td>\n      <td>29</td>\n      <td>0.100000</td>\n      <td>7</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.6853</td>\n      <td>0.7725</td>\n      <td>40</td>\n      <td>XGBRegressor(base_score=None, booster='gbtree'...</td>\n      <td>427</td>\n      <td>20</td>\n      <td>0.000039</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">30</th>\n      <th>1</th>\n      <td>0.6888</td>\n      <td>0.7745</td>\n      <td>217</td>\n      <td>XGBRegressor(base_score=None, booster='gbtree'...</td>\n      <td>427</td>\n      <td>88</td>\n      <td>0.015918</td>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.6909</td>\n      <td>0.7744</td>\n      <td>48</td>\n      <td>XGBRegressor(base_score=None, booster='gbtree'...</td>\n      <td>427</td>\n      <td>20</td>\n      <td>0.100000</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">300</th>\n      <th rowspan=\"2\" valign=\"top\">10</th>\n      <th>1</th>\n      <td>0.6938</td>\n      <td>0.7794</td>\n      <td>702</td>\n      <td>XGBRegressor(base_score=None, booster='gbtree'...</td>\n      <td>427</td>\n      <td>92</td>\n      <td>0.000541</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.6988</td>\n      <td>0.7811</td>\n      <td>99</td>\n      <td>XGBRegressor(base_score=None, booster='gbtree'...</td>\n      <td>427</td>\n      <td>20</td>\n      <td>0.000801</td>\n      <td>7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">30</th>\n      <th>1</th>\n      <td>0.6962</td>\n      <td>0.7818</td>\n      <td>698</td>\n      <td>XGBRegressor(base_score=None, booster='gbtree'...</td>\n      <td>427</td>\n      <td>120</td>\n      <td>0.000010</td>\n      <td>7</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.6968</td>\n      <td>0.7810</td>\n      <td>95</td>\n      <td>XGBRegressor(base_score=None, booster='gbtree'...</td>\n      <td>427</td>\n      <td>136</td>\n      <td>0.000011</td>\n      <td>7</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results,best_model_index=Train_Test_loop(window_size,resolution,stride,model,inner_params, train_paths,val_paths,test_paths,mod_identifier_path='1to2',verbose=1)\n",
    "\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# SETTING OF PARAMETERS FOR THE LOOP FUNCTION\n",
    "# Mike Optimization:\n",
    "\n",
    "window_size=[100,200,500]\n",
    "resolution=[5,10,20]\n",
    "stride=[5,10]\n",
    "\n",
    "train_paths=[bed_file_paths_X1,train_info_X1_path, train_y_X1_path]\n",
    "val_paths=[bed_file_paths_X1,val_info_X1_path, val_y_X1_path]\n",
    "test_paths=[bed_file_paths_X2,val_info_X2_path, val_y_X2_path]\n",
    "\n",
    "\n",
    "model=xgb.XGBRegressor(booster='gbtree',objective='rank:pairwise')\n",
    "\n",
    "inner_params={\n",
    "    'n_estimators': Integer(1,50),\n",
    "    'learning_rate': Real(1e-5,1e-1,prior='log-uniform'),\n",
    "    'max_depth': Integer(1,10),\n",
    "    'reg_lambda':Integer(1,100)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 out of 18\n",
      "WINDOW: 100, RESOLUTION: 5, STRIDE:5\n"
     ]
    }
   ],
   "source": [
    "results,best_model_index=Train_Test_loop(window_size,resolution,stride,model,inner_params, train_paths,val_paths,test_paths,mod_identifier_path='1to2_m_1.0',verbose=1)\n",
    "\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# To do:\n",
    "* Modify the feature extraction function so that it uses the values in bed files (not a binary encoding) even though in that case how do we aggregate?\n",
    "* Try other models: SVMs and Random Forests\n",
    "* Run a loop (or more) for exploring reasonable outer paramters (window_sizes, resolutions, strides)\n",
    "* Implement a convolutional neural network"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Potential param_grid for SVM and RandomForests/ExtraTree (See where optimal Hp are usually located and change the grid in case we have results at the edge)\n",
    "\n",
    "param_grid = {\n",
    "    'C': Real(1e-3, 1e+3, prior='log-uniform'),\n",
    "    'gamma': Real(1e-3, 1e3, prior='log-uniform')\n",
    "}\n",
    "\n",
    "# Note that random forests usually the more estimators we have the better it is (overfitting is quite rare with bagging, I would start with 100 and increase in case of not satisfying results).\n",
    "# Moreover they don't need a lot of tuning so decrease the n_iter in the BayesSearch (we could add this as paramter for the loop)\n",
    "\n",
    "\n",
    "param_grid={'criterion':Categorical(['squared_error', 'absolute_error', 'friedman_mse',]),\n",
    "            'max_features': Categorical(['sqrt','log2']),\n",
    "            'max_depth': Integer:(1,20)}"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
